{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"arro3","text":"<p>A minimal Python library for Apache Arrow, binding to the Rust Arrow implementation.</p> <p>arro3 features:</p> <ul> <li>Classes to manage and operate on Arrow data.</li> <li>Streaming-capable readers and writers for Parquet, Arrow IPC, JSON, and CSV.</li> <li>Streaming compute functions. All relevant compute functions accept streams of input data and return a stream of output data. This means you can transform larger-than-memory data files</li> </ul>"},{"location":"#install","title":"Install","text":"<p>arro3 is distributed with namespace packaging, meaning that individual submodules are distributed separately to PyPI and can be used in isolation.</p> <pre><code>pip install arro3-core arro3-io arro3-compute\n</code></pre> <p>arro3 is also on Conda and can be installed with pixi</p> <pre><code>pixi add arro3-core arro3-io arro3-compute\n</code></pre>"},{"location":"#using","title":"Using","text":"<p>Consult the documentation.</p>"},{"location":"#why-another-arrow-library","title":"Why another Arrow library?","text":"<p>pyarrow is the reference Arrow implementation in Python, and is generally great, but there are a few reasons for <code>arro3</code> to exist:</p> <ul> <li>Lightweight. on MacOS, pyarrow is 100MB on disk, plus 35MB for its required numpy dependency. <code>arro3-core</code> is around 7MB on disk with no required dependencies.</li> <li> <p>Minimal. The core library (<code>arro3-core</code>) has a smaller scope than pyarrow. It includes classes to manage and operate on Arrow data, including <code>Table</code>, <code>RecordBatch</code>, <code>Array</code>, <code>ChunkedArray</code>, <code>RecordBatchReader</code>, <code>Schema</code>, <code>Field</code>, <code>DataType</code>. But, for example, it has a single <code>Array</code> class, while pyarrow has an <code>Int8Array</code>, <code>Int16Array</code>, and so on.</p> <p><code>arro3-core</code> will likely not grow much over time. Other functionality, such as file format readers and writers and compute kernels, will be distributed in other namespace packages, such as <code>arro3-io</code> and <code>arro3-compute</code>. - Modular. The Arrow PyCapsule Interface makes it easy to create small Arrow libraries that communicate via zero-copy data transfer. arro3's Python functions accept Arrow data from any Python library that implements the Arrow PyCapsule Interface, including <code>pyarrow</code>, <code>polars</code> (v1.2+), <code>pandas</code> (v2.2+), <code>nanoarrow</code>, and more.</p> <p>Every functional API in arro3 accepts Arrow data from any Python library. So you can pass a <code>pyarrow.Table</code> directly into <code>arro3.io.write_parquet</code>, and it'll just work.</p> </li> <li> <p>Extensible. arro3 and its sister library pyo3-arrow make it easier for Rust Arrow libraries to be exported to Python. Over time, arro3 can connect to more compute kernels provided by the Rust Arrow implementation as well as .</p> </li> <li>Compliant. Full support for the Arrow specification, including extension types. (Arrow's new view types will be supported from the next Rust <code>arrow</code> release).</li> <li> <p>Streaming-first. All compute and IO functionality is streaming-based with lazy iterators, so you can work with larger-than-memory data.</p> <p>For example, <code>arro3.io.read_parquet</code> returns a <code>RecordBatchReader</code>, an iterator that yields Arrow RecordBatches. This <code>RecordBatchReader</code> can then be passed into any compute function to transform to another <code>RecordBatchReader</code>, that in turn can be passed into <code>arro3.io.write_parquet</code>, at which point both iterators are used.</p> <p>Note that if you do want to materialize data in memory, you should call <code>RecordBatchReader.read_all()</code> or pass the <code>RecordBatchReader</code> to <code>arro3.core.Table()</code>.</p> </li> <li> <p>Pyodide support. arro3 works in Pyodide, a WebAssembly version of Python, and can integrate with other Python packages that implement the Arrow PyCapsule Interface without a pyarrow dependency.</p> </li> <li>Type hints. Type hints are provided for all functionality, making it easier to code in an IDE.</li> </ul>"},{"location":"#drawbacks","title":"Drawbacks","text":"<p>In general, arro3 wraps what already exists in arrow-rs. This ensures that arro3 has a reasonable maintenance burden.</p> <p>arro3 shies away from implementing complete conversion of arbitrary Python objects (or pandas DataFrames) to Arrow. This is complex and well served by other libraries (e.g. pyarrow). But arro3 should provide a minimal and efficient toolbox for to interoperate with other Arrow-compatible libraries.</p>"},{"location":"#using-from-rust","title":"Using from Rust","text":"<p>You can use pyo3-arrow to simplify passing Arrow data between Rust and Python. Refer to its documentation.</p>"},{"location":"api/compute/","title":"arro3.compute","text":""},{"location":"api/compute/#arro3.compute","title":"arro3.compute","text":""},{"location":"api/compute/#arro3.compute.add","title":"add","text":"<pre><code>add(lhs: ArrayInput, rhs: ArrayInput) -&gt; Array\n</code></pre> <p>Perform <code>lhs + rhs</code>, returning an error on overflow</p>"},{"location":"api/compute/#arro3.compute.add_wrapping","title":"add_wrapping","text":"<pre><code>add_wrapping(lhs: ArrayInput, rhs: ArrayInput) -&gt; Array\n</code></pre> <p>Perform <code>lhs + rhs</code>, wrapping on overflow for integer data types.</p>"},{"location":"api/compute/#arro3.compute.can_cast_types","title":"can_cast_types","text":"<pre><code>can_cast_types(\n    from_type: ArrowSchemaExportable, to_type: ArrowSchemaExportable\n) -&gt; bool\n</code></pre> <p>Return true if a value of type <code>from_type</code> can be cast into a value of <code>to_type</code>.</p> <p>Parameters:</p> <ul> <li> <code>from_type</code>               (<code>ArrowSchemaExportable</code>)           \u2013            <p>Source type</p> </li> <li> <code>to_type</code>               (<code>ArrowSchemaExportable</code>)           \u2013            <p>Destination type</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if can be casted.</p> </li> </ul>"},{"location":"api/compute/#arro3.compute.cast","title":"cast","text":"<pre><code>cast(\n    input: ArrayInput | ArrowStreamExportable, to_type: ArrowSchemaExportable\n) -&gt; Array | ArrayReader\n</code></pre> <p>Cast <code>input</code> to the provided data type and return a new Array with type <code>to_type</code>, if possible.</p> <p>If <code>input</code> is an Array, an <code>Array</code> will be returned. If <code>input</code> is a <code>ChunkedArray</code> or <code>ArrayReader</code>, an <code>ArrayReader</code> will be returned.</p> <p>Parameters:</p> <ul> <li> <code>input</code>               (<code>ArrayInput | ArrowStreamExportable</code>)           \u2013            <p>Input data to cast.</p> </li> <li> <code>to_type</code>               (<code>ArrowSchemaExportable</code>)           \u2013            <p>The target data type to cast to. You may pass in a <code>Field</code> here if you wish to include Arrow extension metadata on the output array.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Array | ArrayReader</code>           \u2013            <p>The casted Arrow data.</p> </li> </ul>"},{"location":"api/compute/#arro3.compute.date_part","title":"date_part","text":"<pre><code>date_part(\n    input: ArrowArrayExportable | ArrowStreamExportable,\n    part: DatePart | DatePartT,\n) -&gt; Array | ArrayReader\n</code></pre> <p>Given an array, return a new array with the extracted [<code>DatePart</code>] as signed 32-bit integer values.</p> Currently only supports temporal types <ul> <li>Date32/Date64</li> <li>Time32/Time64</li> <li>Timestamp</li> <li>Interval</li> <li>Duration</li> </ul> <p>Returns an int32-typed array unless input was a dictionary type, in which case returns the dictionary but with this function applied onto its values.</p> <p>If array passed in is not of the above listed types (or is a dictionary array where the values array isn't of the above listed types), then this function will return an error.</p> <p>Parameters:</p> <ul> <li> <code>array</code>           \u2013            <p>Argument to compute function.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Array | ArrayReader</code>           \u2013            <p>The extracted date part.</p> </li> </ul>"},{"location":"api/compute/#arro3.compute.dictionary_encode","title":"dictionary_encode","text":"<pre><code>dictionary_encode(\n    array: ArrayInput | ArrowStreamExportable,\n) -&gt; Array | ArrayReader\n</code></pre> <p>Dictionary-encode array.</p> <p>Return a dictionary-encoded version of the input array. This function does nothing if the input is already a dictionary array.</p> <p>Note: for stream input, each output array will not necessarily have the same dictionary.</p> <p>Parameters:</p> <ul> <li> <code>array</code>               (<code>ArrayInput | ArrowStreamExportable</code>)           \u2013            <p>Argument to compute function.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Array | ArrayReader</code>           \u2013            <p>The dictionary-encoded array.</p> </li> </ul>"},{"location":"api/compute/#arro3.compute.div","title":"div","text":"<pre><code>div(lhs: ArrayInput, rhs: ArrayInput) -&gt; Array\n</code></pre> <p>Perform <code>lhs / rhs</code></p>"},{"location":"api/compute/#arro3.compute.filter","title":"filter","text":"<pre><code>filter(\n    values: ArrayInput | ArrowStreamExportable,\n    predicate: ArrayInput | ArrowStreamExportable,\n) -&gt; Array | ArrayReader\n</code></pre> <p>Returns a filtered <code>values</code> array where the corresponding elements of <code>predicate</code> are <code>true</code>.</p> <p>If <code>input</code> is an Array, an <code>Array</code> will be returned. If <code>input</code> is a <code>ChunkedArray</code> or <code>ArrayReader</code>, an <code>ArrayReader</code> will be returned.</p>"},{"location":"api/compute/#arro3.compute.is_not_null","title":"is_not_null","text":"<pre><code>is_not_null(input: ArrayInput | ArrowStreamExportable) -&gt; Array | ArrayReader\n</code></pre> <p>Returns a non-null boolean-typed array with whether each value of the array is not null.</p> <p>If <code>input</code> is an Array, an <code>Array</code> will be returned. If <code>input</code> is a <code>ChunkedArray</code> or <code>ArrayReader</code>, an <code>ArrayReader</code> will be returned.</p> <p>Parameters:</p> <ul> <li> <code>input</code>               (<code>ArrayInput | ArrowStreamExportable</code>)           \u2013            <p>Input data</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Array | ArrayReader</code>           \u2013            <p>Output</p> </li> </ul>"},{"location":"api/compute/#arro3.compute.is_null","title":"is_null","text":"<pre><code>is_null(input: ArrayInput | ArrowStreamExportable) -&gt; Array | ArrayReader\n</code></pre> <p>Returns a non-null boolean-typed array with whether each value of the array is null.</p> <p>If <code>input</code> is an Array, an <code>Array</code> will be returned. If <code>input</code> is a <code>ChunkedArray</code> or <code>ArrayReader</code>, an <code>ArrayReader</code> will be returned.</p> <p>Parameters:</p> <ul> <li> <code>input</code>               (<code>ArrayInput | ArrowStreamExportable</code>)           \u2013            <p>Input data</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Array | ArrayReader</code>           \u2013            <p>Output</p> </li> </ul>"},{"location":"api/compute/#arro3.compute.max","title":"max","text":"<pre><code>max(input: ArrayInput | ArrowStreamExportable) -&gt; Scalar\n</code></pre> <p>Returns the max of values in the array.</p>"},{"location":"api/compute/#arro3.compute.min","title":"min","text":"<pre><code>min(input: ArrayInput | ArrowStreamExportable) -&gt; Scalar\n</code></pre> <p>Returns the min of values in the array.</p>"},{"location":"api/compute/#arro3.compute.mul","title":"mul","text":"<pre><code>mul(lhs: ArrayInput, rhs: ArrayInput) -&gt; Array\n</code></pre> <p>Perform <code>lhs * rhs</code>, returning an error on overflow</p>"},{"location":"api/compute/#arro3.compute.mul_wrapping","title":"mul_wrapping","text":"<pre><code>mul_wrapping(lhs: ArrayInput, rhs: ArrayInput) -&gt; Array\n</code></pre> <p>Perform <code>lhs * rhs</code>, wrapping on overflow for integer data types.</p>"},{"location":"api/compute/#arro3.compute.neg","title":"neg","text":"<pre><code>neg(array: ArrayInput) -&gt; Array\n</code></pre> <p>Negates each element of array, returning an error on overflow</p>"},{"location":"api/compute/#arro3.compute.neg_wrapping","title":"neg_wrapping","text":"<pre><code>neg_wrapping(array: ArrayInput) -&gt; Array\n</code></pre> <p>Negates each element of array, wrapping on overflow for integer data types.</p>"},{"location":"api/compute/#arro3.compute.rem","title":"rem","text":"<pre><code>rem(lhs: ArrayInput, rhs: ArrayInput) -&gt; Array\n</code></pre> <p>Perform <code>lhs % rhs</code></p>"},{"location":"api/compute/#arro3.compute.sub","title":"sub","text":"<pre><code>sub(lhs: ArrayInput, rhs: ArrayInput) -&gt; Array\n</code></pre> <p>Perform <code>lhs - rhs</code>, returning an error on overflow</p>"},{"location":"api/compute/#arro3.compute.sub_wrapping","title":"sub_wrapping","text":"<pre><code>sub_wrapping(lhs: ArrayInput, rhs: ArrayInput) -&gt; Array\n</code></pre> <p>Perform <code>lhs - rhs</code>, wrapping on overflow for integer data types.</p>"},{"location":"api/compute/#arro3.compute.sum","title":"sum","text":"<pre><code>sum(input: ArrayInput | ArrowStreamExportable) -&gt; Scalar\n</code></pre> <p>Returns the sum of values in the array.</p>"},{"location":"api/compute/#arro3.compute.take","title":"take","text":"<pre><code>take(values: ArrayInput, indices: ArrayInput) -&gt; Array\n</code></pre> <p>Take elements by index from Array, creating a new Array from those indexes.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502        A        \u2502      \u2502    0    \u2502                              \u2502        A        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524      \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524                              \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502        D        \u2502      \u2502    2    \u2502                              \u2502        B        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524      \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524   take(values, indices)      \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502        B        \u2502      \u2502    3    \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6   \u2502        C        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524      \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524                              \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502        C        \u2502      \u2502    1    \u2502                              \u2502        D        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502        E        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nvalues array             indices array                            result\n</code></pre> <p>Parameters:</p> <ul> <li> <code>values</code>               (<code>ArrayInput</code>)           \u2013            <p>The input Arrow data to select from.</p> </li> <li> <code>indices</code>               (<code>ArrayInput</code>)           \u2013            <p>The indices within <code>values</code> to take. This must be a numeric array.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Array</code>           \u2013            <p>The selected arrow data.</p> </li> </ul>"},{"location":"api/io/","title":"arro3.io","text":""},{"location":"api/io/#arro3.io","title":"arro3.io","text":""},{"location":"api/io/#arro3.io.ParquetColumnPath","title":"ParquetColumnPath  <code>module-attribute</code>","text":"<pre><code>ParquetColumnPath = str | Sequence[str]\n</code></pre> <p>Allowed types to refer to a Parquet Column.</p>"},{"location":"api/io/#arro3.io.ParquetCompression","title":"ParquetCompression  <code>module-attribute</code>","text":"<pre><code>ParquetCompression = (\n    Literal[\n        \"uncompressed\",\n        \"snappy\",\n        \"gzip\",\n        \"lzo\",\n        \"brotli\",\n        \"lz4\",\n        \"zstd\",\n        \"lz4_raw\",\n    ]\n    | str\n)\n</code></pre> <p>Allowed compression schemes for Parquet.</p>"},{"location":"api/io/#arro3.io.ParquetEncoding","title":"ParquetEncoding  <code>module-attribute</code>","text":"<pre><code>ParquetEncoding = Literal[\n    \"plain\",\n    \"plain_dictionary\",\n    \"rle\",\n    \"bit_packed\",\n    \"delta_binary_packed\",\n    \"delta_length_byte_array\",\n    \"delta_byte_array\",\n    \"rle_dictionary\",\n    \"byte_stream_split\",\n]\n</code></pre> <p>Allowed Parquet encodings.</p>"},{"location":"api/io/#arro3.io.infer_csv_schema","title":"infer_csv_schema","text":"<pre><code>infer_csv_schema(\n    file: IO[bytes] | Path | str,\n    *,\n    has_header: bool | None = None,\n    max_records: int | None = None,\n    delimiter: str | None = None,\n    escape: str | None = None,\n    quote: str | None = None,\n    terminator: str | None = None,\n    comment: str | None = None\n) -&gt; Schema\n</code></pre> <p>Infer a CSV file's schema</p> <p>If <code>max_records</code> is <code>None</code>, all records will be read, otherwise up to <code>max_records</code> records are read to infer the schema</p> <p>Parameters:</p> <ul> <li> <code>file</code>               (<code>IO[bytes] | Path | str</code>)           \u2013            <p>The input CSV path or buffer.</p> </li> <li> <code>has_header</code>               (<code>bool | None</code>, default:                   <code>None</code> )           \u2013            <p>Set whether the CSV file has a header. Defaults to None.</p> </li> <li> <code>max_records</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The maximum number of records to read to infer schema. Defaults to None.</p> </li> <li> <code>delimiter</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Set the CSV file's column delimiter as a byte character. Defaults to None.</p> </li> <li> <code>escape</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Set the CSV escape character. Defaults to None.</p> </li> <li> <code>quote</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Set the CSV quote character. Defaults to None.</p> </li> <li> <code>terminator</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Set the line terminator. Defaults to None.</p> </li> <li> <code>comment</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Set the comment character. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Schema</code>           \u2013            <p>inferred schema from data</p> </li> </ul>"},{"location":"api/io/#arro3.io.infer_json_schema","title":"infer_json_schema","text":"<pre><code>infer_json_schema(\n    file: IO[bytes] | Path | str, *, max_records: int | None = None\n) -&gt; Schema\n</code></pre> <p>Infer the schema of a JSON file by reading the first n records of the buffer, with <code>max_records</code> controlling the maximum number of records to read.</p> <p>Parameters:</p> <ul> <li> <code>file</code>               (<code>IO[bytes] | Path | str</code>)           \u2013            <p>The input JSON path or buffer.</p> </li> <li> <code>max_records</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The maximum number of records to read to infer schema. If not provided, will read the entire file to deduce field types. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Schema</code>           \u2013            <p>Inferred Arrow Schema</p> </li> </ul>"},{"location":"api/io/#arro3.io.read_csv","title":"read_csv","text":"<pre><code>read_csv(\n    file: IO[bytes] | Path | str,\n    schema: ArrowSchemaExportable,\n    *,\n    has_header: bool | None = None,\n    batch_size: int | None = None,\n    delimiter: str | None = None,\n    escape: str | None = None,\n    quote: str | None = None,\n    terminator: str | None = None,\n    comment: str | None = None\n) -&gt; RecordBatchReader\n</code></pre> <p>Read a CSV file to an Arrow RecordBatchReader.</p> <p>Parameters:</p> <ul> <li> <code>file</code>               (<code>IO[bytes] | Path | str</code>)           \u2013            <p>The input CSV path or buffer.</p> </li> <li> <code>schema</code>               (<code>ArrowSchemaExportable</code>)           \u2013            <p>The Arrow schema for this CSV file. Use infer_csv_schema to infer an Arrow schema if</p> </li> <li> <code>has_header</code>               (<code>bool | None</code>, default:                   <code>None</code> )           \u2013            <p>Set whether the CSV file has a header. Defaults to None.</p> </li> <li> <code>batch_size</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Set the batch size (number of records to load at one time). Defaults to None.</p> </li> <li> <code>delimiter</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Set the CSV file's column delimiter as a byte character. Defaults to None.</p> </li> <li> <code>escape</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Set the CSV escape character. Defaults to None.</p> </li> <li> <code>quote</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Set the CSV quote character. Defaults to None.</p> </li> <li> <code>terminator</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Set the line terminator. Defaults to None.</p> </li> <li> <code>comment</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Set the comment character. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>RecordBatchReader</code>           \u2013            <p>A RecordBatchReader with read CSV data</p> </li> </ul>"},{"location":"api/io/#arro3.io.read_ipc","title":"read_ipc","text":"<pre><code>read_ipc(file: IO[bytes] | Path | str) -&gt; RecordBatchReader\n</code></pre> <p>Read an Arrow IPC file into memory</p> <p>Parameters:</p> <ul> <li> <code>file</code>               (<code>IO[bytes] | Path | str</code>)           \u2013            <p>The input Arrow IPC file path or buffer.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>RecordBatchReader</code>           \u2013            <p>An arrow RecordBatchReader.</p> </li> </ul>"},{"location":"api/io/#arro3.io.read_ipc_stream","title":"read_ipc_stream","text":"<pre><code>read_ipc_stream(file: IO[bytes] | Path | str) -&gt; RecordBatchReader\n</code></pre> <p>Read an Arrow IPC stream into memory</p> <p>Parameters:</p> <ul> <li> <code>file</code>               (<code>IO[bytes] | Path | str</code>)           \u2013            <p>The input Arrow IPC stream path or buffer.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>RecordBatchReader</code>           \u2013            <p>An arrow RecordBatchReader.</p> </li> </ul>"},{"location":"api/io/#arro3.io.read_json","title":"read_json","text":"<pre><code>read_json(\n    file: IO[bytes] | Path | str,\n    schema: ArrowSchemaExportable,\n    *,\n    batch_size: int | None = None\n) -&gt; RecordBatchReader\n</code></pre> <p>Reads JSON data with a known schema into Arrow</p> <p>Parameters:</p> <ul> <li> <code>file</code>               (<code>IO[bytes] | Path | str</code>)           \u2013            <p>The JSON file or buffer to read from.</p> </li> <li> <code>schema</code>               (<code>ArrowSchemaExportable</code>)           \u2013            <p>The Arrow schema representing the JSON data.</p> </li> <li> <code>batch_size</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Set the batch size (number of records to load at one time). Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>RecordBatchReader</code>           \u2013            <p>An arrow RecordBatchReader.</p> </li> </ul>"},{"location":"api/io/#arro3.io.read_parquet","title":"read_parquet","text":"<pre><code>read_parquet(file: IO[bytes] | Path | str) -&gt; RecordBatchReader\n</code></pre> <p>Read a Parquet file to an Arrow RecordBatchReader</p> <p>Parameters:</p> <ul> <li> <code>file</code>               (<code>IO[bytes] | Path | str</code>)           \u2013            <p>The input Parquet file path or buffer.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>RecordBatchReader</code>           \u2013            <p>The loaded Arrow data.</p> </li> </ul>"},{"location":"api/io/#arro3.io.write_csv","title":"write_csv","text":"<pre><code>write_csv(\n    data: ArrowStreamExportable | ArrowArrayExportable,\n    file: IO[bytes] | Path | str,\n    *,\n    header: bool | None = None,\n    delimiter: str | None = None,\n    escape: str | None = None,\n    quote: str | None = None,\n    date_format: str | None = None,\n    datetime_format: str | None = None,\n    time_format: str | None = None,\n    timestamp_format: str | None = None,\n    timestamp_tz_format: str | None = None,\n    null: str | None = None\n) -&gt; None\n</code></pre> <p>Write an Arrow Table or stream to a CSV file.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>ArrowStreamExportable | ArrowArrayExportable</code>)           \u2013            <p>The Arrow Table, RecordBatchReader, or RecordBatch to write.</p> </li> <li> <code>file</code>               (<code>IO[bytes] | Path | str</code>)           \u2013            <p>The output buffer or file path for where to write the CSV.</p> </li> <li> <code>header</code>               (<code>bool | None</code>, default:                   <code>None</code> )           \u2013            <p>Set whether to write the CSV file with a header. Defaults to None.</p> </li> <li> <code>delimiter</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Set the CSV file's column delimiter as a byte character. Defaults to None.</p> </li> <li> <code>escape</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Set the CSV file's escape character as a byte character.</p> <p>In some variants of CSV, quotes are escaped using a special escape character like <code>\\</code> (instead of escaping quotes by doubling them).</p> <p>By default, writing these idiosyncratic escapes is disabled, and is only used when double_quote is disabled. Defaults to None.</p> </li> <li> <code>quote</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Set the CSV file's quote character as a byte character. Defaults to None.</p> </li> <li> <code>date_format</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Set the CSV file's date format. Defaults to None.</p> </li> <li> <code>datetime_format</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Set the CSV file's datetime format. Defaults to None.</p> </li> <li> <code>time_format</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Set the CSV file's time format. Defaults to None.</p> </li> <li> <code>timestamp_format</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Set the CSV file's timestamp format. Defaults to None.</p> </li> <li> <code>timestamp_tz_format</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Set the CSV file's timestamp tz format. Defaults to None.</p> </li> <li> <code>null</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Set the value to represent null in output. Defaults to None.</p> </li> </ul>"},{"location":"api/io/#arro3.io.write_ipc","title":"write_ipc","text":"<pre><code>write_ipc(\n    data: ArrowStreamExportable | ArrowArrayExportable,\n    file: IO[bytes] | Path | str,\n    *,\n    compression: Literal[\"LZ4\", \"lz4\", \"ZSTD\", \"zstd\"] | None = None\n) -&gt; None\n</code></pre> <p>Write Arrow data to an Arrow IPC file</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>ArrowStreamExportable | ArrowArrayExportable</code>)           \u2013            <p>the Arrow Table, RecordBatchReader, or RecordBatch to write.</p> </li> <li> <code>file</code>               (<code>IO[bytes] | Path | str</code>)           \u2013            <p>the output file or buffer to write to</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>compression</code>               (<code>Literal['LZ4', 'lz4', 'ZSTD', 'zstd'] | None</code>)           \u2013            <p>Compression to apply to file.</p> </li> </ul>"},{"location":"api/io/#arro3.io.write_ipc_stream","title":"write_ipc_stream","text":"<pre><code>write_ipc_stream(\n    data: ArrowStreamExportable | ArrowArrayExportable,\n    file: IO[bytes] | Path | str,\n    *,\n    compression: Literal[\"LZ4\", \"lz4\", \"ZSTD\", \"zstd\"] | None = None\n) -&gt; None\n</code></pre> <p>Write Arrow data to an Arrow IPC stream</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>ArrowStreamExportable | ArrowArrayExportable</code>)           \u2013            <p>the Arrow Table, RecordBatchReader, or RecordBatch to write.</p> </li> <li> <code>file</code>               (<code>IO[bytes] | Path | str</code>)           \u2013            <p>the output file or buffer to write to</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>compression</code>               (<code>Literal['LZ4', 'lz4', 'ZSTD', 'zstd'] | None</code>)           \u2013            <p>Compression to apply to file.</p> </li> </ul>"},{"location":"api/io/#arro3.io.write_json","title":"write_json","text":"<pre><code>write_json(\n    data: ArrowStreamExportable | ArrowArrayExportable,\n    file: IO[bytes] | Path | str,\n    *,\n    explicit_nulls: bool | None = None\n) -&gt; None\n</code></pre> <p>Write Arrow data to JSON.</p> <p>By default the writer will skip writing keys with null values for backward compatibility.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>ArrowStreamExportable | ArrowArrayExportable</code>)           \u2013            <p>the Arrow Table, RecordBatchReader, or RecordBatch to write.</p> </li> <li> <code>file</code>               (<code>IO[bytes] | Path | str</code>)           \u2013            <p>the output file or buffer to write to</p> </li> <li> <code>explicit_nulls</code>               (<code>bool | None</code>, default:                   <code>None</code> )           \u2013            <p>Set whether to keep keys with null values, or to omit writing them. Defaults to skipping nulls.</p> </li> </ul>"},{"location":"api/io/#arro3.io.write_ndjson","title":"write_ndjson","text":"<pre><code>write_ndjson(\n    data: ArrowStreamExportable | ArrowArrayExportable,\n    file: IO[bytes] | Path | str,\n    *,\n    explicit_nulls: bool | None = None\n) -&gt; None\n</code></pre> <p>Write Arrow data to newline-delimited JSON.</p> <p>By default the writer will skip writing keys with null values for backward compatibility.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>ArrowStreamExportable | ArrowArrayExportable</code>)           \u2013            <p>the Arrow Table, RecordBatchReader, or RecordBatch to write.</p> </li> <li> <code>file</code>               (<code>IO[bytes] | Path | str</code>)           \u2013            <p>the output file or buffer to write to</p> </li> <li> <code>explicit_nulls</code>               (<code>bool | None</code>, default:                   <code>None</code> )           \u2013            <p>Set whether to keep keys with null values, or to omit writing them. Defaults to skipping nulls.</p> </li> </ul>"},{"location":"api/io/#arro3.io.write_parquet","title":"write_parquet","text":"<pre><code>write_parquet(\n    data: ArrowStreamExportable | ArrowArrayExportable,\n    file: IO[bytes] | Path | str,\n    *,\n    bloom_filter_enabled: bool | None = None,\n    bloom_filter_fpp: float | None = None,\n    bloom_filter_ndv: int | None = None,\n    column_compression: dict[ParquetColumnPath, ParquetCompression]\n    | None = None,\n    column_dictionary_enabled: dict[ParquetColumnPath, bool] | None = None,\n    column_encoding: dict[ParquetColumnPath, ParquetEncoding] | None = None,\n    column_max_statistics_size: dict[ParquetColumnPath, int] | None = None,\n    compression: ParquetCompression | None = None,\n    created_by: str | None = None,\n    data_page_row_count_limit: int | None = None,\n    data_page_size_limit: int | None = None,\n    dictionary_enabled: bool | None = None,\n    dictionary_page_size_limit: int | None = None,\n    encoding: ParquetEncoding | None = None,\n    key_value_metadata: dict[str, str] | None = None,\n    max_row_group_size: int | None = None,\n    max_statistics_size: int | None = None,\n    skip_arrow_metadata: bool = False,\n    write_batch_size: int | None = None,\n    writer_version: Literal[\"parquet_1_0\", \"parquet_2_0\"] | None = None\n) -&gt; None\n</code></pre> <p>Write an Arrow Table or stream to a Parquet file.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>ArrowStreamExportable | ArrowArrayExportable</code>)           \u2013            <p>The Arrow Table, RecordBatchReader, or RecordBatch to write.</p> </li> <li> <code>file</code>               (<code>IO[bytes] | Path | str</code>)           \u2013            <p>The output file.</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>bloom_filter_enabled</code>               (<code>bool | None</code>)           \u2013            <p>Sets if bloom filter is enabled by default for all columns (defaults to <code>false</code>).</p> </li> <li> <code>bloom_filter_fpp</code>               (<code>float | None</code>)           \u2013            <p>Sets the default target bloom filter false positive probability (fpp) for all columns (defaults to <code>0.05</code>).</p> </li> <li> <code>bloom_filter_ndv</code>               (<code>int | None</code>)           \u2013            <p>Sets default number of distinct values (ndv) for bloom filter for all columns (defaults to <code>1_000_000</code>).</p> </li> <li> <code>column_compression</code>               (<code>dict[ParquetColumnPath, ParquetCompression] | None</code>)           \u2013            <p>Sets compression codec for a specific column. Takes precedence over <code>compression</code>.</p> </li> <li> <code>column_dictionary_enabled</code>               (<code>dict[ParquetColumnPath, bool] | None</code>)           \u2013            <p>Sets flag to enable/disable dictionary encoding for a specific column. Takes precedence over <code>dictionary_enabled</code>.</p> </li> <li> <code>column_encoding</code>               (<code>dict[ParquetColumnPath, ParquetEncoding] | None</code>)           \u2013            <p>Sets encoding for a specific column. Takes precedence over <code>encoding</code>.</p> </li> <li> <code>column_max_statistics_size</code>               (<code>dict[ParquetColumnPath, int] | None</code>)           \u2013            <p>Sets max size for statistics for a specific column. Takes precedence over <code>max_statistics_size</code>.</p> </li> <li> <code>compression</code>               (<code>ParquetCompression | None</code>)           \u2013            <p>Sets default compression codec for all columns (default to <code>uncompressed</code>). Note that you can pass in a custom compression level with a string like <code>\"zstd(3)\"</code> or <code>\"gzip(9)\"</code> or <code>\"brotli(3)\"</code>.</p> </li> <li> <code>created_by</code>               (<code>str | None</code>)           \u2013            <p>Sets \"created by\" property (defaults to <code>parquet-rs version &lt;VERSION&gt;</code>).</p> </li> <li> <code>data_page_row_count_limit</code>               (<code>int | None</code>)           \u2013            <p>Sets best effort maximum number of rows in a data page (defaults to <code>20_000</code>).</p> <p>The parquet writer will attempt to limit the number of rows in each <code>DataPage</code> to this value. Reducing this value will result in larger parquet files, but may improve the effectiveness of page index based predicate pushdown during reading.</p> <p>Note: this is a best effort limit based on value of <code>set_write_batch_size</code>.</p> </li> <li> <code>data_page_size_limit</code>               (<code>int | None</code>)           \u2013            <p>Sets best effort maximum size of a data page in bytes (defaults to <code>1024 * 1024</code>).</p> <p>The parquet writer will attempt to limit the sizes of each <code>DataPage</code> to this many bytes. Reducing this value will result in larger parquet files, but may improve the effectiveness of page index based predicate pushdown during reading.</p> <p>Note: this is a best effort limit based on value of <code>set_write_batch_size</code>.</p> </li> <li> <code>dictionary_enabled</code>               (<code>bool | None</code>)           \u2013            <p>Sets default flag to enable/disable dictionary encoding for all columns (defaults to <code>True</code>).</p> </li> <li> <code>dictionary_page_size_limit</code>               (<code>int | None</code>)           \u2013            <p>Sets best effort maximum dictionary page size, in bytes (defaults to <code>1024 * 1024</code>).</p> <p>The parquet writer will attempt to limit the size of each <code>DataPage</code> used to store dictionaries to this many bytes. Reducing this value will result in larger parquet files, but may improve the effectiveness of page index based predicate pushdown during reading.</p> <p>Note: this is a best effort limit based on value of <code>set_write_batch_size</code>.</p> </li> <li> <code>encoding</code>               (<code>ParquetEncoding | None</code>)           \u2013            <p>Sets default encoding for all columns.</p> <p>If dictionary is not enabled, this is treated as a primary encoding for all columns. In case when dictionary is enabled for any column, this value is considered to be a fallback encoding for that column.</p> </li> <li> <code>key_value_metadata</code>               (<code>dict[str, str] | None</code>)           \u2013            <p>Sets \"key_value_metadata\" property (defaults to <code>None</code>).</p> </li> <li> <code>max_row_group_size</code>               (<code>int | None</code>)           \u2013            <p>Sets maximum number of rows in a row group (defaults to <code>1024 * 1024</code>).</p> </li> <li> <code>max_statistics_size</code>               (<code>int | None</code>)           \u2013            <p>Sets default max statistics size for all columns (defaults to <code>4096</code>).</p> </li> <li> <code>skip_arrow_metadata</code>               (<code>bool</code>)           \u2013            <p>Parquet files generated by this writer contain embedded arrow schema by default. Set <code>skip_arrow_metadata</code> to <code>True</code>, to skip encoding the embedded metadata (defaults to <code>False</code>).</p> </li> <li> <code>write_batch_size</code>               (<code>int | None</code>)           \u2013            <p>Sets write batch size (defaults to 1024).</p> <p>For performance reasons, data for each column is written in batches of this size.</p> <p>Additional limits such as such as <code>set_data_page_row_count_limit</code> are checked between batches, and thus the write batch size value acts as an upper-bound on the enforcement granularity of other limits.</p> </li> <li> <code>writer_version</code>               (<code>Literal['parquet_1_0', 'parquet_2_0'] | None</code>)           \u2013            <p>Sets the <code>WriterVersion</code> written into the parquet metadata (defaults to <code>\"parquet_1_0\"</code>). This value can determine what features some readers will support.</p> </li> </ul>"},{"location":"api/core/accessors/","title":"Accessors","text":""},{"location":"api/core/accessors/#arro3.core","title":"arro3.core","text":""},{"location":"api/core/accessors/#arro3.core.dictionary_dictionary","title":"dictionary_dictionary","text":"<pre><code>dictionary_dictionary(\n    array: ArrowArrayExportable | ArrowStreamExportable,\n) -&gt; Array | ArrayReader\n</code></pre> <p>Access the <code>dictionary</code> of a dictionary array.</p> <p>This is equivalent to the <code>.dictionary</code> attribute on a PyArrow DictionaryArray.</p> <p>Parameters:</p> <ul> <li> <code>array</code>               (<code>ArrowArrayExportable | ArrowStreamExportable</code>)           \u2013            <p>Argument to compute function.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Array | ArrayReader</code>           \u2013            <p>The keys of a dictionary-encoded array.</p> </li> </ul>"},{"location":"api/core/accessors/#arro3.core.dictionary_indices","title":"dictionary_indices","text":"<pre><code>dictionary_indices(\n    array: ArrowArrayExportable | ArrowStreamExportable,\n) -&gt; Array | ArrayReader\n</code></pre> <p>Access the indices of a dictionary array.</p> <p>This is equivalent to the <code>.indices</code> attribute on a PyArrow DictionaryArray.</p> <p>Parameters:</p> <ul> <li> <code>array</code>               (<code>ArrowArrayExportable | ArrowStreamExportable</code>)           \u2013            <p>Argument to compute function.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Array | ArrayReader</code>           \u2013            <p>The indices of a dictionary-encoded array.</p> </li> </ul>"},{"location":"api/core/accessors/#arro3.core.list_flatten","title":"list_flatten","text":"<pre><code>list_flatten(\n    input: ArrowArrayExportable | ArrowStreamExportable,\n) -&gt; Array | ArrayReader\n</code></pre> <p>Unnest this ListArray, LargeListArray or FixedSizeListArray.</p> <p>Parameters:</p> <ul> <li> <code>input</code>               (<code>ArrowArrayExportable | ArrowStreamExportable</code>)           \u2013            <p>Input data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Array | ArrayReader</code>           \u2013            <p>The flattened Arrow data.</p> </li> </ul>"},{"location":"api/core/accessors/#arro3.core.list_offsets","title":"list_offsets","text":"<pre><code>list_offsets(\n    input: ArrowArrayExportable | ArrowStreamExportable, *, logical: bool = True\n) -&gt; Array | ArrayReader\n</code></pre> <p>Access the offsets of this ListArray or LargeListArray</p> <p>Parameters:</p> <ul> <li> <code>input</code>               (<code>ArrowArrayExportable | ArrowStreamExportable</code>)           \u2013            <p>description</p> </li> <li> <code>physical</code>           \u2013            <p>If False, return the physical (unsliced) offsets of the provided list array. If True, adjust the list offsets for the current array slicing. Defaults to <code>True</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Array | ArrayReader</code>           \u2013            <p>description</p> </li> </ul>"},{"location":"api/core/accessors/#arro3.core.struct_field","title":"struct_field","text":"<pre><code>struct_field(\n    values: ArrowArrayExportable, /, indices: int | Sequence[int]\n) -&gt; Array\n</code></pre> <p>Access a column within a StructArray by index</p> <p>Parameters:</p> <ul> <li> <code>values</code>               (<code>ArrowArrayExportable</code>)           \u2013            <p>Argument to compute function.</p> </li> <li> <code>indices</code>               (<code>int | Sequence[int]</code>)           \u2013            <p>List of indices for chained field lookup, for example [4, 1] will look up the second nested field in the fifth outer field.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Array</code>           \u2013            <p>description</p> </li> </ul>"},{"location":"api/core/array-reader/","title":"ArrayReader","text":""},{"location":"api/core/array-reader/#arro3.core.ArrayReader","title":"arro3.core.ArrayReader","text":"<p>A stream of Arrow <code>Array</code>s.</p> <p>This is similar to the <code>RecordBatchReader</code> but each item yielded from the stream is an <code>Array</code>, not a <code>RecordBatch</code>.</p>"},{"location":"api/core/array-reader/#arro3.core.ArrayReader.closed","title":"closed  <code>property</code>","text":"<pre><code>closed: bool\n</code></pre> <p>Returns <code>true</code> if this reader has already been consumed.</p>"},{"location":"api/core/array-reader/#arro3.core.ArrayReader.field","title":"field  <code>property</code>","text":"<pre><code>field: Field\n</code></pre> <p>Access the field of this reader.</p>"},{"location":"api/core/array-reader/#arro3.core.ArrayReader.__arrow_c_schema__","title":"__arrow_c_schema__","text":"<pre><code>__arrow_c_schema__() -&gt; object\n</code></pre> <p>An implementation of the Arrow PyCapsule Interface. This dunder method should not be called directly, but enables zero-copy data transfer to other Python libraries that understand Arrow memory.</p> <p>This allows Arrow consumers to inspect the data type of this ArrayReader. Then the consumer can ask the producer (in <code>__arrow_c_stream__</code>) to cast the exported data to a supported data type.</p>"},{"location":"api/core/array-reader/#arro3.core.ArrayReader.__arrow_c_stream__","title":"__arrow_c_stream__","text":"<pre><code>__arrow_c_stream__(requested_schema: object | None = None) -&gt; object\n</code></pre> <p>An implementation of the Arrow PyCapsule Interface. This dunder method should not be called directly, but enables zero-copy data transfer to other Python libraries that understand Arrow memory.</p> <p>For example, you can call <code>pyarrow.chunked_array()</code> to convert this ArrayReader to a pyarrow ChunkedArray, without copying memory.</p>"},{"location":"api/core/array-reader/#arro3.core.ArrayReader.from_arrays","title":"from_arrays  <code>classmethod</code>","text":"<pre><code>from_arrays(\n    field: ArrowSchemaExportable, arrays: Sequence[ArrowArrayExportable]\n) -&gt; ArrayReader\n</code></pre> <p>Construct an ArrayReader from existing data.</p> <p>Parameters:</p> <ul> <li> <code>field</code>               (<code>ArrowSchemaExportable</code>)           \u2013            <p>The Arrow field that describes the sequence of array data.</p> </li> <li> <code>arrays</code>               (<code>Sequence[ArrowArrayExportable]</code>)           \u2013            <p>A sequence (list or tuple) of Array data.</p> </li> </ul>"},{"location":"api/core/array-reader/#arro3.core.ArrayReader.from_arrow","title":"from_arrow  <code>classmethod</code>","text":"<pre><code>from_arrow(input: ArrowArrayExportable | ArrowStreamExportable) -&gt; ArrayReader\n</code></pre> <p>Construct this from an existing Arrow object.</p> <p>It can be called on anything that exports the Arrow stream interface (has an <code>__arrow_c_stream__</code> method), such as a <code>Table</code> or <code>ArrayReader</code>.</p>"},{"location":"api/core/array-reader/#arro3.core.ArrayReader.from_arrow_pycapsule","title":"from_arrow_pycapsule  <code>classmethod</code>","text":"<pre><code>from_arrow_pycapsule(capsule) -&gt; ArrayReader\n</code></pre> <p>Construct this object from a bare Arrow PyCapsule</p>"},{"location":"api/core/array-reader/#arro3.core.ArrayReader.from_stream","title":"from_stream  <code>classmethod</code>","text":"<pre><code>from_stream(data: ArrowStreamExportable) -&gt; ArrayReader\n</code></pre> <p>Construct this from an existing Arrow object.</p> <p>This is an alias of and has the same behavior as <code>from_arrow</code>, but is included for parity with <code>pyarrow.RecordBatchReader</code>.</p>"},{"location":"api/core/array-reader/#arro3.core.ArrayReader.read_all","title":"read_all","text":"<pre><code>read_all() -&gt; ChunkedArray\n</code></pre> <p>Read all batches from this stream into a ChunkedArray.</p>"},{"location":"api/core/array-reader/#arro3.core.ArrayReader.read_next_array","title":"read_next_array","text":"<pre><code>read_next_array() -&gt; Array\n</code></pre> <p>Read the next array from this stream.</p>"},{"location":"api/core/array/","title":"Array","text":""},{"location":"api/core/array/#arro3.core.Array","title":"arro3.core.Array","text":"<p>An Arrow Array.</p>"},{"location":"api/core/array/#arro3.core.Array.field","title":"field  <code>property</code>","text":"<pre><code>field: Field\n</code></pre> <p>Access the field stored on this Array.</p> <p>Note that this field usually will not have a name associated, but it may have metadata that signifies that this array is an extension (user-defined typed) array.</p>"},{"location":"api/core/array/#arro3.core.Array.nbytes","title":"nbytes  <code>property</code>","text":"<pre><code>nbytes: int\n</code></pre> <p>The number of bytes in this Array.</p>"},{"location":"api/core/array/#arro3.core.Array.null_count","title":"null_count  <code>property</code>","text":"<pre><code>null_count: int\n</code></pre> <p>The number of null values in this Array.</p>"},{"location":"api/core/array/#arro3.core.Array.type","title":"type  <code>property</code>","text":"<pre><code>type: DataType\n</code></pre> <p>The data type of this array.</p>"},{"location":"api/core/array/#arro3.core.Array.__arrow_c_array__","title":"__arrow_c_array__","text":"<pre><code>__arrow_c_array__(\n    requested_schema: object | None = None,\n) -&gt; tuple[object, object]\n</code></pre> <p>An implementation of the Arrow PyCapsule Interface. This dunder method should not be called directly, but enables zero-copy data transfer to other Python libraries that understand Arrow memory.</p> <p>For example, you can call <code>pyarrow.array()</code> to convert this array into a pyarrow array, without copying memory.</p>"},{"location":"api/core/array/#arro3.core.Array.__arrow_c_schema__","title":"__arrow_c_schema__","text":"<pre><code>__arrow_c_schema__() -&gt; object\n</code></pre> <p>An implementation of the Arrow PyCapsule Interface. This dunder method should not be called directly, but enables zero-copy data transfer to other Python libraries that understand Arrow memory.</p> <p>This allows Arrow consumers to inspect the data type of this array. Then the consumer can ask the producer (in <code>__arrow_c_array__</code>) to cast the exported data to a supported data type.</p>"},{"location":"api/core/array/#arro3.core.Array.cast","title":"cast","text":"<pre><code>cast(target_type: ArrowSchemaExportable) -&gt; Array\n</code></pre> <p>Cast array values to another data type</p> <p>Parameters:</p> <ul> <li> <code>target_type</code>               (<code>ArrowSchemaExportable</code>)           \u2013            <p>Type to cast array to.</p> </li> </ul>"},{"location":"api/core/array/#arro3.core.Array.from_arrow","title":"from_arrow  <code>classmethod</code>","text":"<pre><code>from_arrow(input: ArrowArrayExportable | ArrowStreamExportable) -&gt; Array\n</code></pre> <p>Construct this object from an existing Arrow object.</p> <p>It can be called on anything that exports the Arrow data interface (<code>__arrow_c_array__</code>).</p> <p>Parameters:</p> <ul> <li> <code>input</code>               (<code>ArrowArrayExportable | ArrowStreamExportable</code>)           \u2013            <p>Arrow array to use for constructing this object</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Array</code>           \u2013            <p>Self</p> </li> </ul>"},{"location":"api/core/array/#arro3.core.Array.from_arrow_pycapsule","title":"from_arrow_pycapsule  <code>classmethod</code>","text":"<pre><code>from_arrow_pycapsule(schema_capsule, array_capsule) -&gt; Array\n</code></pre> <p>Construct this object from bare Arrow PyCapsules</p>"},{"location":"api/core/array/#arro3.core.Array.from_buffer","title":"from_buffer  <code>classmethod</code>","text":"<pre><code>from_buffer(buffer: BufferProtocolExportable | Any) -&gt; Array\n</code></pre> <p>Construct an Array from an object implementing the Python Buffer Protocol.</p>"},{"location":"api/core/array/#arro3.core.Array.from_numpy","title":"from_numpy  <code>classmethod</code>","text":"<pre><code>from_numpy(array: ndarray) -&gt; Array\n</code></pre> <p>Construct an Array from a numpy ndarray</p>"},{"location":"api/core/array/#arro3.core.Array.slice","title":"slice","text":"<pre><code>slice(offset: int = 0, length: int | None = None) -&gt; Array\n</code></pre> <p>Compute zero-copy slice of this array.</p> <p>Parameters:</p> <ul> <li> <code>offset</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Defaults to 0.</p> </li> <li> <code>length</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Array</code>           \u2013            <p>The sliced array</p> </li> </ul>"},{"location":"api/core/array/#arro3.core.Array.take","title":"take","text":"<pre><code>take(indices: ArrayInput) -&gt; Array\n</code></pre> <p>Take specific indices from this Array.</p>"},{"location":"api/core/array/#arro3.core.Array.to_numpy","title":"to_numpy","text":"<pre><code>to_numpy() -&gt; NDArray\n</code></pre> <p>Return a numpy copy of this array.</p>"},{"location":"api/core/array/#arro3.core.Array.to_pylist","title":"to_pylist","text":"<pre><code>to_pylist() -&gt; NDArray\n</code></pre> <p>Convert to a list of native Python objects.</p>"},{"location":"api/core/chunked-array/","title":"ChunkedArray","text":""},{"location":"api/core/chunked-array/#arro3.core.ChunkedArray","title":"arro3.core.ChunkedArray","text":"<p>An Arrow ChunkedArray.</p>"},{"location":"api/core/chunked-array/#arro3.core.ChunkedArray.chunks","title":"chunks  <code>property</code>","text":"<pre><code>chunks: list[Array]\n</code></pre> <p>Convert to a list of single-chunked arrays.</p>"},{"location":"api/core/chunked-array/#arro3.core.ChunkedArray.field","title":"field  <code>property</code>","text":"<pre><code>field: Field\n</code></pre> <p>Access the field stored on this ChunkedArray.</p> <p>Note that this field usually will not have a name associated, but it may have metadata that signifies that this array is an extension (user-defined typed) array.</p>"},{"location":"api/core/chunked-array/#arro3.core.ChunkedArray.nbytes","title":"nbytes  <code>property</code>","text":"<pre><code>nbytes: int\n</code></pre> <p>Total number of bytes consumed by the elements of the chunked array.</p>"},{"location":"api/core/chunked-array/#arro3.core.ChunkedArray.null_count","title":"null_count  <code>property</code>","text":"<pre><code>null_count: int\n</code></pre> <p>Number of null entries</p>"},{"location":"api/core/chunked-array/#arro3.core.ChunkedArray.num_chunks","title":"num_chunks  <code>property</code>","text":"<pre><code>num_chunks: int\n</code></pre> <p>Number of underlying chunks.</p>"},{"location":"api/core/chunked-array/#arro3.core.ChunkedArray.type","title":"type  <code>property</code>","text":"<pre><code>type: DataType\n</code></pre> <p>Return data type of a ChunkedArray.</p>"},{"location":"api/core/chunked-array/#arro3.core.ChunkedArray.__arrow_c_schema__","title":"__arrow_c_schema__","text":"<pre><code>__arrow_c_schema__() -&gt; object\n</code></pre> <p>An implementation of the Arrow PyCapsule Interface. This dunder method should not be called directly, but enables zero-copy data transfer to other Python libraries that understand Arrow memory.</p> <p>This allows Arrow consumers to inspect the data type of this ChunkedArray. Then the consumer can ask the producer (in <code>__arrow_c_stream__</code>) to cast the exported data to a supported data type.</p>"},{"location":"api/core/chunked-array/#arro3.core.ChunkedArray.__arrow_c_stream__","title":"__arrow_c_stream__","text":"<pre><code>__arrow_c_stream__(requested_schema: object | None = None) -&gt; object\n</code></pre> <p>An implementation of the Arrow PyCapsule Interface. This dunder method should not be called directly, but enables zero-copy data transfer to other Python libraries that understand Arrow memory.</p> <p>For example (as of pyarrow v16), you can call <code>pyarrow.chunked_array()</code> to convert this array into a pyarrow array, without copying memory.</p>"},{"location":"api/core/chunked-array/#arro3.core.ChunkedArray.cast","title":"cast","text":"<pre><code>cast(target_type: ArrowSchemaExportable) -&gt; ChunkedArray\n</code></pre> <p>Cast array values to another data type</p> <p>Parameters:</p> <ul> <li> <code>target_type</code>               (<code>ArrowSchemaExportable</code>)           \u2013            <p>Type to cast array to.</p> </li> </ul>"},{"location":"api/core/chunked-array/#arro3.core.ChunkedArray.chunk","title":"chunk","text":"<pre><code>chunk(i: int) -&gt; Array\n</code></pre> <p>Select a chunk by its index.</p> <p>Parameters:</p> <ul> <li> <code>i</code>               (<code>int</code>)           \u2013            <p>chunk index.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Array</code>           \u2013            <p>new Array.</p> </li> </ul>"},{"location":"api/core/chunked-array/#arro3.core.ChunkedArray.combine_chunks","title":"combine_chunks","text":"<pre><code>combine_chunks() -&gt; Array\n</code></pre> <p>Flatten this ChunkedArray into a single non-chunked array.</p>"},{"location":"api/core/chunked-array/#arro3.core.ChunkedArray.equals","title":"equals","text":"<pre><code>equals(other: ArrowStreamExportable) -&gt; bool\n</code></pre> <p>Return whether the contents of two chunked arrays are equal.</p>"},{"location":"api/core/chunked-array/#arro3.core.ChunkedArray.from_arrow","title":"from_arrow  <code>classmethod</code>","text":"<pre><code>from_arrow(input: ArrowArrayExportable | ArrowStreamExportable) -&gt; ChunkedArray\n</code></pre> <p>Construct this from an existing Arrow object.</p> <p>It can be called on anything that exports the Arrow stream interface (has an <code>__arrow_c_stream__</code> method). All batches from the stream will be materialized in memory.</p>"},{"location":"api/core/chunked-array/#arro3.core.ChunkedArray.from_arrow_pycapsule","title":"from_arrow_pycapsule  <code>classmethod</code>","text":"<pre><code>from_arrow_pycapsule(capsule) -&gt; ChunkedArray\n</code></pre> <p>Construct this object from a bare Arrow PyCapsule</p>"},{"location":"api/core/chunked-array/#arro3.core.ChunkedArray.length","title":"length","text":"<pre><code>length() -&gt; int\n</code></pre> <p>Return length of a ChunkedArray.</p>"},{"location":"api/core/chunked-array/#arro3.core.ChunkedArray.rechunk","title":"rechunk","text":"<pre><code>rechunk(*, max_chunksize: int | None = None) -&gt; ChunkedArray\n</code></pre> <p>Rechunk a ChunkedArray with a maximum number of rows per chunk.</p> <p>Parameters:</p> <ul> <li> <code>max_chunksize</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The maximum number of rows per internal array. Defaults to None, which rechunks into a single array.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ChunkedArray</code>           \u2013            <p>The rechunked ChunkedArray.</p> </li> </ul>"},{"location":"api/core/chunked-array/#arro3.core.ChunkedArray.slice","title":"slice","text":"<pre><code>slice(offset: int = 0, length: int | None = None) -&gt; ChunkedArray\n</code></pre> <p>Compute zero-copy slice of this ChunkedArray</p> <p>Parameters:</p> <ul> <li> <code>offset</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Offset from start of array to slice. Defaults to 0.</p> </li> <li> <code>length</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Length of slice (default is until end of batch starting from offset).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ChunkedArray</code>           \u2013            <p>New ChunkedArray</p> </li> </ul>"},{"location":"api/core/chunked-array/#arro3.core.ChunkedArray.to_numpy","title":"to_numpy","text":"<pre><code>to_numpy() -&gt; NDArray\n</code></pre> <p>Copy this array to a <code>numpy</code> NDArray</p>"},{"location":"api/core/chunked-array/#arro3.core.ChunkedArray.to_pylist","title":"to_pylist","text":"<pre><code>to_pylist() -&gt; NDArray\n</code></pre> <p>Convert to a list of native Python objects.</p>"},{"location":"api/core/constructors/","title":"Constructors","text":""},{"location":"api/core/constructors/#arro3.core","title":"arro3.core","text":""},{"location":"api/core/constructors/#arro3.core.fixed_size_list_array","title":"fixed_size_list_array","text":"<pre><code>fixed_size_list_array(\n    values: ArrayInput,\n    list_size: int,\n    *,\n    type: ArrowSchemaExportable | None = None\n) -&gt; Array\n</code></pre> <p>Construct a new fixed size list array</p> <p>Parameters:</p> <ul> <li> <code>values</code>               (<code>ArrayInput</code>)           \u2013            <p>the values of the new fixed size list array</p> </li> <li> <code>list_size</code>               (<code>int</code>)           \u2013            <p>the number of elements in each item of the list.</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>type</code>               (<code>ArrowSchemaExportable | None</code>)           \u2013            <p>the type of output array. This must have fixed size list type. You may pass a <code>Field</code> into this parameter to associate extension metadata with the created array. Defaults to None, in which case it is inferred.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Array</code>           \u2013            <p>a new Array with fixed size list type</p> </li> </ul>"},{"location":"api/core/constructors/#arro3.core.list_array","title":"list_array","text":"<pre><code>list_array(\n    offsets: ArrayInput,\n    values: ArrayInput,\n    *,\n    type: ArrowSchemaExportable | None = None\n) -&gt; Array\n</code></pre> <p>Construct a new list array</p> <p>Parameters:</p> <ul> <li> <code>offsets</code>               (<code>ArrayInput</code>)           \u2013            <p>the offsets for the output list array. This array must have type int32 or int64, depending on whether you wish to create a list array or large list array.</p> </li> <li> <code>values</code>               (<code>ArrayInput</code>)           \u2013            <p>the values for the output list array.</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>type</code>               (<code>ArrowSchemaExportable | None</code>)           \u2013            <p>the type of output array. This must have list or large list type. You may pass a <code>Field</code> into this parameter to associate extension metadata with the created array. Defaults to None, in which case it is inferred.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Array</code>           \u2013            <p>a new Array with list or large list type</p> </li> </ul>"},{"location":"api/core/constructors/#arro3.core.struct_array","title":"struct_array","text":"<pre><code>struct_array(\n    arrays: Sequence[ArrayInput],\n    *,\n    fields: Sequence[ArrowSchemaExportable],\n    type: ArrowSchemaExportable | None = None\n) -&gt; Array\n</code></pre> <p>Construct a new struct array</p> <p>Parameters:</p> <ul> <li> <code>arrays</code>               (<code>Sequence[ArrayInput]</code>)           \u2013            <p>a sequence of arrays for the struct children</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>fields</code>               (<code>Sequence[ArrowSchemaExportable]</code>)           \u2013            <p>a sequence of fields that represent each of the struct children</p> </li> <li> <code>type</code>               (<code>ArrowSchemaExportable | None</code>)           \u2013            <p>the type of output array. This must have struct type. You may pass a <code>Field</code> into this parameter to associate extension metadata with the created array. Defaults to None, in which case it is inferred .</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Array</code>           \u2013            <p>a new Array with struct type</p> </li> </ul>"},{"location":"api/core/datatype/","title":"DataType","text":""},{"location":"api/core/datatype/#arro3.core.DataType","title":"arro3.core.DataType","text":"<p>An Arrow DataType.</p>"},{"location":"api/core/datatype/#arro3.core.DataType.bit_width","title":"bit_width  <code>property</code>","text":"<pre><code>bit_width: Literal[8, 16, 32, 64] | None\n</code></pre> <p>Returns the bit width of this type if it is a primitive type</p> <p>Returns <code>None</code> if not a primitive type</p>"},{"location":"api/core/datatype/#arro3.core.DataType.list_size","title":"list_size  <code>property</code>","text":"<pre><code>list_size: int | None\n</code></pre> <p>The size of the list in the case of fixed size lists.</p> <p>This will return <code>None</code> if the data type is not a fixed size list.</p> <p>Examples:</p> <pre><code>from arro3.core import DataType\nDataType.list(DataType.int32(), 2).list_size\n# 2\n</code></pre> <p>Returns:</p> <ul> <li> <code>int | None</code>           \u2013            <p>description</p> </li> </ul>"},{"location":"api/core/datatype/#arro3.core.DataType.num_fields","title":"num_fields  <code>property</code>","text":"<pre><code>num_fields: int\n</code></pre> <p>The number of child fields.</p>"},{"location":"api/core/datatype/#arro3.core.DataType.time_unit","title":"time_unit  <code>property</code>","text":"<pre><code>time_unit: Literal['s', 'ms', 'us', 'ns'] | None\n</code></pre> <p>The time unit, if the data type has one.</p>"},{"location":"api/core/datatype/#arro3.core.DataType.tz","title":"tz  <code>property</code>","text":"<pre><code>tz: str | None\n</code></pre> <p>The timestamp time zone, if any, or None.</p>"},{"location":"api/core/datatype/#arro3.core.DataType.value_type","title":"value_type  <code>property</code>","text":"<pre><code>value_type: DataType | None\n</code></pre> <p>The child type, if it exists.</p>"},{"location":"api/core/datatype/#arro3.core.DataType.__arrow_c_schema__","title":"__arrow_c_schema__","text":"<pre><code>__arrow_c_schema__() -&gt; object\n</code></pre> <p>An implementation of the Arrow PyCapsule Interface. This dunder method should not be called directly, but enables zero-copy data transfer to other Python libraries that understand Arrow memory.</p> <p>For example, you can call <code>pyarrow.field()</code> to convert this array into a pyarrow field, without copying memory.</p>"},{"location":"api/core/datatype/#arro3.core.DataType.binary","title":"binary  <code>classmethod</code>","text":"<pre><code>binary(length: int | None = None) -&gt; DataType\n</code></pre> <p>Create variable-length or fixed size binary type.</p> <p>Parameters:</p> <ul> <li> <code>length</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>If length is <code>None</code> then return a variable length binary type. If length is provided, then return a fixed size binary type of width <code>length</code>. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>description</p> </li> </ul>"},{"location":"api/core/datatype/#arro3.core.DataType.binary_view","title":"binary_view  <code>classmethod</code>","text":"<pre><code>binary_view() -&gt; DataType\n</code></pre> <p>Create a variable-length binary view type.</p>"},{"location":"api/core/datatype/#arro3.core.DataType.bool","title":"bool  <code>classmethod</code>","text":"<pre><code>bool() -&gt; DataType\n</code></pre> <p>Create instance of boolean type.</p>"},{"location":"api/core/datatype/#arro3.core.DataType.date32","title":"date32  <code>classmethod</code>","text":"<pre><code>date32() -&gt; DataType\n</code></pre> <p>Create instance of 32-bit date (days since UNIX epoch 1970-01-01).</p>"},{"location":"api/core/datatype/#arro3.core.DataType.date64","title":"date64  <code>classmethod</code>","text":"<pre><code>date64() -&gt; DataType\n</code></pre> <p>Create instance of 64-bit date (milliseconds since UNIX epoch 1970-01-01).</p>"},{"location":"api/core/datatype/#arro3.core.DataType.decimal128","title":"decimal128  <code>classmethod</code>","text":"<pre><code>decimal128(precision: int, scale: int) -&gt; DataType\n</code></pre> <p>Create decimal type with precision and scale and 128-bit width.</p> <p>Arrow decimals are fixed-point decimal numbers encoded as a scaled integer. The precision is the number of significant digits that the decimal type can represent; the scale is the number of digits after the decimal point (note the scale can be negative).</p> <p>As an example, <code>decimal128(7, 3)</code> can exactly represent the numbers 1234.567 and -1234.567 (encoded internally as the 128-bit integers 1234567 and -1234567, respectively), but neither 12345.67 nor 123.4567.</p> <p><code>decimal128(5, -3)</code> can exactly represent the number 12345000 (encoded internally as the 128-bit integer 12345), but neither 123450000 nor 1234500.</p> <p>If you need a precision higher than 38 significant digits, consider using <code>decimal256</code>.</p> <p>Parameters:</p> <ul> <li> <code>precision</code>               (<code>int</code>)           \u2013            <p>Must be between 1 and 38 scale: description</p> </li> </ul>"},{"location":"api/core/datatype/#arro3.core.DataType.decimal256","title":"decimal256  <code>classmethod</code>","text":"<pre><code>decimal256(precision: int, scale: int) -&gt; DataType\n</code></pre> <p>Create decimal type with precision and scale and 256-bit width.</p>"},{"location":"api/core/datatype/#arro3.core.DataType.dictionary","title":"dictionary  <code>classmethod</code>","text":"<pre><code>dictionary(\n    index_type: ArrowSchemaExportable, value_type: ArrowSchemaExportable\n) -&gt; DataType\n</code></pre> <p>Dictionary (categorical, or simply encoded) type.</p> <p>Parameters:</p> <ul> <li> <code>index_type</code>               (<code>ArrowSchemaExportable</code>)           \u2013            <p>description</p> </li> <li> <code>value_type</code>               (<code>ArrowSchemaExportable</code>)           \u2013            <p>description</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>description</p> </li> </ul>"},{"location":"api/core/datatype/#arro3.core.DataType.duration","title":"duration  <code>classmethod</code>","text":"<pre><code>duration(unit: Literal['s', 'ms', 'us', 'ns']) -&gt; DataType\n</code></pre> <p>Create instance of a duration type with unit resolution.</p> <p>Parameters:</p> <ul> <li> <code>unit</code>               (<code>Literal['s', 'ms', 'us', 'ns']</code>)           \u2013            <p>one of <code>'s'</code> [second], <code>'ms'</code> [millisecond], <code>'us'</code> [microsecond], or <code>'ns'</code> [nanosecond]</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>description</p> </li> </ul>"},{"location":"api/core/datatype/#arro3.core.DataType.equals","title":"equals","text":"<pre><code>equals(other: ArrowSchemaExportable, *, check_metadata: bool = False) -&gt; bool\n</code></pre> <p>Return true if type is equivalent to passed value.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>ArrowSchemaExportable</code>)           \u2013            <p>description</p> </li> <li> <code>check_metadata</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether nested Field metadata equality should be checked as well. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>description</p> </li> </ul>"},{"location":"api/core/datatype/#arro3.core.DataType.float16","title":"float16  <code>classmethod</code>","text":"<pre><code>float16() -&gt; DataType\n</code></pre> <p>Create half-precision floating point type.</p>"},{"location":"api/core/datatype/#arro3.core.DataType.float32","title":"float32  <code>classmethod</code>","text":"<pre><code>float32() -&gt; DataType\n</code></pre> <p>Create single-precision floating point type.</p>"},{"location":"api/core/datatype/#arro3.core.DataType.float64","title":"float64  <code>classmethod</code>","text":"<pre><code>float64() -&gt; DataType\n</code></pre> <p>Create double-precision floating point type.</p>"},{"location":"api/core/datatype/#arro3.core.DataType.from_arrow","title":"from_arrow  <code>classmethod</code>","text":"<pre><code>from_arrow(input: ArrowSchemaExportable) -&gt; DataType\n</code></pre> <p>Construct this from an existing Arrow object.</p> <p>It can be called on anything that exports the Arrow schema interface (has an <code>__arrow_c_schema__</code> method).</p>"},{"location":"api/core/datatype/#arro3.core.DataType.from_arrow_pycapsule","title":"from_arrow_pycapsule  <code>classmethod</code>","text":"<pre><code>from_arrow_pycapsule(capsule) -&gt; DataType\n</code></pre> <p>Construct this object from a bare Arrow PyCapsule</p>"},{"location":"api/core/datatype/#arro3.core.DataType.int16","title":"int16  <code>classmethod</code>","text":"<pre><code>int16() -&gt; DataType\n</code></pre> <p>Create instance of signed int16 type.</p>"},{"location":"api/core/datatype/#arro3.core.DataType.int32","title":"int32  <code>classmethod</code>","text":"<pre><code>int32() -&gt; DataType\n</code></pre> <p>Create instance of signed int32 type.</p>"},{"location":"api/core/datatype/#arro3.core.DataType.int64","title":"int64  <code>classmethod</code>","text":"<pre><code>int64() -&gt; DataType\n</code></pre> <p>Create instance of signed int64 type.</p>"},{"location":"api/core/datatype/#arro3.core.DataType.int8","title":"int8  <code>classmethod</code>","text":"<pre><code>int8() -&gt; DataType\n</code></pre> <p>Create instance of signed int8 type.</p>"},{"location":"api/core/datatype/#arro3.core.DataType.is_binary","title":"is_binary  <code>staticmethod</code>","text":"<pre><code>is_binary(t: ArrowSchemaExportable) -&gt; bool\n</code></pre>"},{"location":"api/core/datatype/#arro3.core.DataType.is_binary_view","title":"is_binary_view  <code>staticmethod</code>","text":"<pre><code>is_binary_view(t: ArrowSchemaExportable) -&gt; bool\n</code></pre>"},{"location":"api/core/datatype/#arro3.core.DataType.is_boolean","title":"is_boolean  <code>staticmethod</code>","text":"<pre><code>is_boolean(t: ArrowSchemaExportable) -&gt; bool\n</code></pre>"},{"location":"api/core/datatype/#arro3.core.DataType.is_date","title":"is_date  <code>staticmethod</code>","text":"<pre><code>is_date(t: ArrowSchemaExportable) -&gt; bool\n</code></pre>"},{"location":"api/core/datatype/#arro3.core.DataType.is_date32","title":"is_date32  <code>staticmethod</code>","text":"<pre><code>is_date32(t: ArrowSchemaExportable) -&gt; bool\n</code></pre>"},{"location":"api/core/datatype/#arro3.core.DataType.is_date64","title":"is_date64  <code>staticmethod</code>","text":"<pre><code>is_date64(t: ArrowSchemaExportable) -&gt; bool\n</code></pre>"},{"location":"api/core/datatype/#arro3.core.DataType.is_decimal","title":"is_decimal  <code>staticmethod</code>","text":"<pre><code>is_decimal(t: ArrowSchemaExportable) -&gt; bool\n</code></pre>"},{"location":"api/core/datatype/#arro3.core.DataType.is_decimal128","title":"is_decimal128  <code>staticmethod</code>","text":"<pre><code>is_decimal128(t: ArrowSchemaExportable) -&gt; bool\n</code></pre>"},{"location":"api/core/datatype/#arro3.core.DataType.is_decimal256","title":"is_decimal256  <code>staticmethod</code>","text":"<pre><code>is_decimal256(t: ArrowSchemaExportable) -&gt; bool\n</code></pre>"},{"location":"api/core/datatype/#arro3.core.DataType.is_dictionary","title":"is_dictionary  <code>staticmethod</code>","text":"<pre><code>is_dictionary(t: ArrowSchemaExportable) -&gt; bool\n</code></pre>"},{"location":"api/core/datatype/#arro3.core.DataType.is_dictionary_key_type","title":"is_dictionary_key_type  <code>staticmethod</code>","text":"<pre><code>is_dictionary_key_type(t: ArrowSchemaExportable) -&gt; bool\n</code></pre>"},{"location":"api/core/datatype/#arro3.core.DataType.is_duration","title":"is_duration  <code>staticmethod</code>","text":"<pre><code>is_duration(t: ArrowSchemaExportable) -&gt; bool\n</code></pre>"},{"location":"api/core/datatype/#arro3.core.DataType.is_fixed_size_binary","title":"is_fixed_size_binary  <code>staticmethod</code>","text":"<pre><code>is_fixed_size_binary(t: ArrowSchemaExportable) -&gt; bool\n</code></pre>"},{"location":"api/core/datatype/#arro3.core.DataType.is_fixed_size_list","title":"is_fixed_size_list  <code>staticmethod</code>","text":"<pre><code>is_fixed_size_list(t: ArrowSchemaExportable) -&gt; bool\n</code></pre>"},{"location":"api/core/datatype/#arro3.core.DataType.is_float16","title":"is_float16  <code>staticmethod</code>","text":"<pre><code>is_float16(t: ArrowSchemaExportable) -&gt; bool\n</code></pre>"},{"location":"api/core/datatype/#arro3.core.DataType.is_float32","title":"is_float32  <code>staticmethod</code>","text":"<pre><code>is_float32(t: ArrowSchemaExportable) -&gt; bool\n</code></pre>"},{"location":"api/core/datatype/#arro3.core.DataType.is_float64","title":"is_float64  <code>staticmethod</code>","text":"<pre><code>is_float64(t: ArrowSchemaExportable) -&gt; bool\n</code></pre>"},{"location":"api/core/datatype/#arro3.core.DataType.is_floating","title":"is_floating  <code>staticmethod</code>","text":"<pre><code>is_floating(t: ArrowSchemaExportable) -&gt; bool\n</code></pre>"},{"location":"api/core/datatype/#arro3.core.DataType.is_int16","title":"is_int16  <code>staticmethod</code>","text":"<pre><code>is_int16(t: ArrowSchemaExportable) -&gt; bool\n</code></pre>"},{"location":"api/core/datatype/#arro3.core.DataType.is_int32","title":"is_int32  <code>staticmethod</code>","text":"<pre><code>is_int32(t: ArrowSchemaExportable) -&gt; bool\n</code></pre>"},{"location":"api/core/datatype/#arro3.core.DataType.is_int64","title":"is_int64  <code>staticmethod</code>","text":"<pre><code>is_int64(t: ArrowSchemaExportable) -&gt; bool\n</code></pre>"},{"location":"api/core/datatype/#arro3.core.DataType.is_int8","title":"is_int8  <code>staticmethod</code>","text":"<pre><code>is_int8(t: ArrowSchemaExportable) -&gt; bool\n</code></pre>"},{"location":"api/core/datatype/#arro3.core.DataType.is_integer","title":"is_integer  <code>staticmethod</code>","text":"<pre><code>is_integer(t: ArrowSchemaExportable) -&gt; bool\n</code></pre>"},{"location":"api/core/datatype/#arro3.core.DataType.is_interval","title":"is_interval  <code>staticmethod</code>","text":"<pre><code>is_interval(t: ArrowSchemaExportable) -&gt; bool\n</code></pre>"},{"location":"api/core/datatype/#arro3.core.DataType.is_large_binary","title":"is_large_binary  <code>staticmethod</code>","text":"<pre><code>is_large_binary(t: ArrowSchemaExportable) -&gt; bool\n</code></pre>"},{"location":"api/core/datatype/#arro3.core.DataType.is_large_list","title":"is_large_list  <code>staticmethod</code>","text":"<pre><code>is_large_list(t: ArrowSchemaExportable) -&gt; bool\n</code></pre>"},{"location":"api/core/datatype/#arro3.core.DataType.is_large_list_view","title":"is_large_list_view  <code>staticmethod</code>","text":"<pre><code>is_large_list_view(t: ArrowSchemaExportable) -&gt; bool\n</code></pre>"},{"location":"api/core/datatype/#arro3.core.DataType.is_large_string","title":"is_large_string  <code>staticmethod</code>","text":"<pre><code>is_large_string(t: ArrowSchemaExportable) -&gt; bool\n</code></pre>"},{"location":"api/core/datatype/#arro3.core.DataType.is_large_unicode","title":"is_large_unicode  <code>staticmethod</code>","text":"<pre><code>is_large_unicode(t: ArrowSchemaExportable) -&gt; bool\n</code></pre>"},{"location":"api/core/datatype/#arro3.core.DataType.is_list","title":"is_list  <code>staticmethod</code>","text":"<pre><code>is_list(t: ArrowSchemaExportable) -&gt; bool\n</code></pre>"},{"location":"api/core/datatype/#arro3.core.DataType.is_list_view","title":"is_list_view  <code>staticmethod</code>","text":"<pre><code>is_list_view(t: ArrowSchemaExportable) -&gt; bool\n</code></pre>"},{"location":"api/core/datatype/#arro3.core.DataType.is_map","title":"is_map  <code>staticmethod</code>","text":"<pre><code>is_map(t: ArrowSchemaExportable) -&gt; bool\n</code></pre>"},{"location":"api/core/datatype/#arro3.core.DataType.is_nested","title":"is_nested  <code>staticmethod</code>","text":"<pre><code>is_nested(t: ArrowSchemaExportable) -&gt; bool\n</code></pre>"},{"location":"api/core/datatype/#arro3.core.DataType.is_null","title":"is_null  <code>staticmethod</code>","text":"<pre><code>is_null(t: ArrowSchemaExportable) -&gt; bool\n</code></pre>"},{"location":"api/core/datatype/#arro3.core.DataType.is_numeric","title":"is_numeric  <code>staticmethod</code>","text":"<pre><code>is_numeric(t: ArrowSchemaExportable) -&gt; bool\n</code></pre>"},{"location":"api/core/datatype/#arro3.core.DataType.is_primitive","title":"is_primitive  <code>staticmethod</code>","text":"<pre><code>is_primitive(t: ArrowSchemaExportable) -&gt; bool\n</code></pre>"},{"location":"api/core/datatype/#arro3.core.DataType.is_run_end_encoded","title":"is_run_end_encoded  <code>staticmethod</code>","text":"<pre><code>is_run_end_encoded(t: ArrowSchemaExportable) -&gt; bool\n</code></pre>"},{"location":"api/core/datatype/#arro3.core.DataType.is_signed_integer","title":"is_signed_integer  <code>staticmethod</code>","text":"<pre><code>is_signed_integer(t: ArrowSchemaExportable) -&gt; bool\n</code></pre>"},{"location":"api/core/datatype/#arro3.core.DataType.is_string","title":"is_string  <code>staticmethod</code>","text":"<pre><code>is_string(t: ArrowSchemaExportable) -&gt; bool\n</code></pre>"},{"location":"api/core/datatype/#arro3.core.DataType.is_string_view","title":"is_string_view  <code>staticmethod</code>","text":"<pre><code>is_string_view(t: ArrowSchemaExportable) -&gt; bool\n</code></pre>"},{"location":"api/core/datatype/#arro3.core.DataType.is_struct","title":"is_struct  <code>staticmethod</code>","text":"<pre><code>is_struct(t: ArrowSchemaExportable) -&gt; bool\n</code></pre>"},{"location":"api/core/datatype/#arro3.core.DataType.is_temporal","title":"is_temporal  <code>staticmethod</code>","text":"<pre><code>is_temporal(t: ArrowSchemaExportable) -&gt; bool\n</code></pre>"},{"location":"api/core/datatype/#arro3.core.DataType.is_time","title":"is_time  <code>staticmethod</code>","text":"<pre><code>is_time(t: ArrowSchemaExportable) -&gt; bool\n</code></pre>"},{"location":"api/core/datatype/#arro3.core.DataType.is_time32","title":"is_time32  <code>staticmethod</code>","text":"<pre><code>is_time32(t: ArrowSchemaExportable) -&gt; bool\n</code></pre>"},{"location":"api/core/datatype/#arro3.core.DataType.is_time64","title":"is_time64  <code>staticmethod</code>","text":"<pre><code>is_time64(t: ArrowSchemaExportable) -&gt; bool\n</code></pre>"},{"location":"api/core/datatype/#arro3.core.DataType.is_timestamp","title":"is_timestamp  <code>staticmethod</code>","text":"<pre><code>is_timestamp(t: ArrowSchemaExportable) -&gt; bool\n</code></pre>"},{"location":"api/core/datatype/#arro3.core.DataType.is_uint16","title":"is_uint16  <code>staticmethod</code>","text":"<pre><code>is_uint16(t: ArrowSchemaExportable) -&gt; bool\n</code></pre>"},{"location":"api/core/datatype/#arro3.core.DataType.is_uint32","title":"is_uint32  <code>staticmethod</code>","text":"<pre><code>is_uint32(t: ArrowSchemaExportable) -&gt; bool\n</code></pre>"},{"location":"api/core/datatype/#arro3.core.DataType.is_uint64","title":"is_uint64  <code>staticmethod</code>","text":"<pre><code>is_uint64(t: ArrowSchemaExportable) -&gt; bool\n</code></pre>"},{"location":"api/core/datatype/#arro3.core.DataType.is_uint8","title":"is_uint8  <code>staticmethod</code>","text":"<pre><code>is_uint8(t: ArrowSchemaExportable) -&gt; bool\n</code></pre>"},{"location":"api/core/datatype/#arro3.core.DataType.is_unicode","title":"is_unicode  <code>staticmethod</code>","text":"<pre><code>is_unicode(t: ArrowSchemaExportable) -&gt; bool\n</code></pre>"},{"location":"api/core/datatype/#arro3.core.DataType.is_union","title":"is_union  <code>staticmethod</code>","text":"<pre><code>is_union(t: ArrowSchemaExportable) -&gt; bool\n</code></pre>"},{"location":"api/core/datatype/#arro3.core.DataType.is_unsigned_integer","title":"is_unsigned_integer  <code>staticmethod</code>","text":"<pre><code>is_unsigned_integer(t: ArrowSchemaExportable) -&gt; bool\n</code></pre>"},{"location":"api/core/datatype/#arro3.core.DataType.large_binary","title":"large_binary  <code>classmethod</code>","text":"<pre><code>large_binary() -&gt; DataType\n</code></pre> <p>Create large variable-length binary type.</p>"},{"location":"api/core/datatype/#arro3.core.DataType.large_list","title":"large_list  <code>classmethod</code>","text":"<pre><code>large_list(value_type: ArrowSchemaExportable) -&gt; DataType\n</code></pre> <p>Create LargeListType instance from child data type or field.</p> <p>This data type may not be supported by all Arrow implementations. Unless you need to represent data larger than 2**31 elements, you should prefer <code>list()</code>.</p> <p>Parameters:</p> <ul> <li> <code>value_type</code>               (<code>ArrowSchemaExportable</code>)           \u2013            <p>description</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>description</p> </li> </ul>"},{"location":"api/core/datatype/#arro3.core.DataType.large_list_view","title":"large_list_view  <code>classmethod</code>","text":"<pre><code>large_list_view(value_type: ArrowSchemaExportable) -&gt; DataType\n</code></pre> <p>Create LargeListViewType instance from child data type or field.</p> <p>This data type may not be supported by all Arrow implementations because it is an alternative to the ListType.</p> <p>Parameters:</p> <ul> <li> <code>value_type</code>               (<code>ArrowSchemaExportable</code>)           \u2013            <p>description</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>description</p> </li> </ul>"},{"location":"api/core/datatype/#arro3.core.DataType.large_string","title":"large_string  <code>classmethod</code>","text":"<pre><code>large_string() -&gt; DataType\n</code></pre> <p>Create large UTF8 variable-length string type.</p>"},{"location":"api/core/datatype/#arro3.core.DataType.large_utf8","title":"large_utf8  <code>classmethod</code>","text":"<pre><code>large_utf8() -&gt; DataType\n</code></pre> <p>Alias for large_string().</p>"},{"location":"api/core/datatype/#arro3.core.DataType.list","title":"list  <code>classmethod</code>","text":"<pre><code>list(\n    value_type: ArrowSchemaExportable, list_size: int | None = None\n) -&gt; DataType\n</code></pre> <p>Create ListType instance from child data type or field.</p> <p>Parameters:</p> <ul> <li> <code>value_type</code>               (<code>ArrowSchemaExportable</code>)           \u2013            <p>description</p> </li> <li> <code>list_size</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>If length is <code>None</code> then return a variable length list type. If length is provided then return a fixed size list type.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>description</p> </li> </ul>"},{"location":"api/core/datatype/#arro3.core.DataType.list_view","title":"list_view  <code>classmethod</code>","text":"<pre><code>list_view(value_type: ArrowSchemaExportable) -&gt; DataType\n</code></pre> <p>Create ListViewType instance from child data type or field.</p> <p>This data type may not be supported by all Arrow implementations because it is an alternative to the ListType.</p>"},{"location":"api/core/datatype/#arro3.core.DataType.map","title":"map  <code>classmethod</code>","text":"<pre><code>map(\n    key_type: ArrowSchemaExportable,\n    item_type: ArrowSchemaExportable,\n    keys_sorted: bool,\n) -&gt; DataType\n</code></pre> <p>Create MapType instance from key and item data types or fields.</p> <p>Parameters:</p> <ul> <li> <code>key_type</code>               (<code>ArrowSchemaExportable</code>)           \u2013            <p>description</p> </li> <li> <code>item_type</code>               (<code>ArrowSchemaExportable</code>)           \u2013            <p>description</p> </li> <li> <code>keys_sorted</code>               (<code>bool</code>)           \u2013            <p>description</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>description</p> </li> </ul>"},{"location":"api/core/datatype/#arro3.core.DataType.month_day_nano_interval","title":"month_day_nano_interval  <code>classmethod</code>","text":"<pre><code>month_day_nano_interval() -&gt; DataType\n</code></pre> <p>Create instance of an interval type representing months, days and nanoseconds between two dates.</p>"},{"location":"api/core/datatype/#arro3.core.DataType.null","title":"null  <code>classmethod</code>","text":"<pre><code>null() -&gt; DataType\n</code></pre> <p>Create instance of null type.</p>"},{"location":"api/core/datatype/#arro3.core.DataType.run_end_encoded","title":"run_end_encoded  <code>classmethod</code>","text":"<pre><code>run_end_encoded(\n    run_end_type: ArrowSchemaExportable, value_type: ArrowSchemaExportable\n) -&gt; DataType\n</code></pre> <p>Create RunEndEncodedType from run-end and value types.</p> <p>Parameters:</p> <ul> <li> <code>run_end_type</code>               (<code>ArrowSchemaExportable</code>)           \u2013            <p>The integer type of the run_ends array. Must be <code>'int16'</code>, <code>'int32'</code>, or <code>'int64'</code>.</p> </li> <li> <code>value_type</code>               (<code>ArrowSchemaExportable</code>)           \u2013            <p>The type of the values array.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>description</p> </li> </ul>"},{"location":"api/core/datatype/#arro3.core.DataType.string","title":"string  <code>classmethod</code>","text":"<pre><code>string() -&gt; DataType\n</code></pre> <p>Create UTF8 variable-length string type.</p>"},{"location":"api/core/datatype/#arro3.core.DataType.string_view","title":"string_view  <code>classmethod</code>","text":"<pre><code>string_view() -&gt; DataType\n</code></pre> <p>Create UTF8 variable-length string view type.</p>"},{"location":"api/core/datatype/#arro3.core.DataType.struct","title":"struct  <code>classmethod</code>","text":"<pre><code>struct(fields: Sequence[ArrowSchemaExportable]) -&gt; DataType\n</code></pre> <p>Create StructType instance from fields.</p> <p>A struct is a nested type parameterized by an ordered sequence of types (which can all be distinct), called its fields.</p> <p>Parameters:</p> <ul> <li> <code>fields</code>               (<code>Sequence[ArrowSchemaExportable]</code>)           \u2013            <p>Each field must have a UTF8-encoded name, and these field names are part of the type metadata.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>description</p> </li> </ul>"},{"location":"api/core/datatype/#arro3.core.DataType.time32","title":"time32  <code>classmethod</code>","text":"<pre><code>time32(unit: Literal['s', 'ms']) -&gt; DataType\n</code></pre> <p>Create instance of 32-bit time (time of day) type with unit resolution.</p> <p>Parameters:</p> <ul> <li> <code>unit</code>               (<code>Literal['s', 'ms']</code>)           \u2013            <p>one of <code>'s'</code> [second], or <code>'ms'</code> [millisecond]</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>description</p> </li> </ul>"},{"location":"api/core/datatype/#arro3.core.DataType.time64","title":"time64  <code>classmethod</code>","text":"<pre><code>time64(unit: Literal['us', 'ns']) -&gt; DataType\n</code></pre> <p>Create instance of 64-bit time (time of day) type with unit resolution.</p> <p>Parameters:</p> <ul> <li> <code>unit</code>               (<code>Literal['us', 'ns']</code>)           \u2013            <p>One of <code>'us'</code> [microsecond], or <code>'ns'</code> [nanosecond].</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>description</p> </li> </ul>"},{"location":"api/core/datatype/#arro3.core.DataType.timestamp","title":"timestamp  <code>classmethod</code>","text":"<pre><code>timestamp(\n    unit: Literal[\"s\", \"ms\", \"us\", \"ns\"], *, tz: str | None = None\n) -&gt; DataType\n</code></pre> <p>Create instance of timestamp type with resolution and optional time zone.</p> <p>Parameters:</p> <ul> <li> <code>unit</code>               (<code>Literal['s', 'ms', 'us', 'ns']</code>)           \u2013            <p>one of <code>'s'</code> [second], <code>'ms'</code> [millisecond], <code>'us'</code> [microsecond], or <code>'ns'</code> [nanosecond]</p> </li> <li> <code>tz</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Time zone name. None indicates time zone naive. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataType</code>           \u2013            <p>description</p> </li> </ul>"},{"location":"api/core/datatype/#arro3.core.DataType.uint16","title":"uint16  <code>classmethod</code>","text":"<pre><code>uint16() -&gt; DataType\n</code></pre> <p>Create instance of unsigned int16 type.</p>"},{"location":"api/core/datatype/#arro3.core.DataType.uint32","title":"uint32  <code>classmethod</code>","text":"<pre><code>uint32() -&gt; DataType\n</code></pre> <p>Create instance of unsigned int32 type.</p>"},{"location":"api/core/datatype/#arro3.core.DataType.uint64","title":"uint64  <code>classmethod</code>","text":"<pre><code>uint64() -&gt; DataType\n</code></pre> <p>Create instance of unsigned int64 type.</p>"},{"location":"api/core/datatype/#arro3.core.DataType.uint8","title":"uint8  <code>classmethod</code>","text":"<pre><code>uint8() -&gt; DataType\n</code></pre> <p>Create instance of unsigned int8 type.</p>"},{"location":"api/core/datatype/#arro3.core.DataType.utf8","title":"utf8  <code>classmethod</code>","text":"<pre><code>utf8() -&gt; DataType\n</code></pre> <p>Alias for string().</p>"},{"location":"api/core/field/","title":"Field","text":""},{"location":"api/core/field/#arro3.core.Field","title":"arro3.core.Field","text":"<p>An Arrow Field.</p>"},{"location":"api/core/field/#arro3.core.Field.metadata","title":"metadata  <code>property</code>","text":"<pre><code>metadata: dict[bytes, bytes]\n</code></pre> <p>The schema's metadata.</p>"},{"location":"api/core/field/#arro3.core.Field.metadata_str","title":"metadata_str  <code>property</code>","text":"<pre><code>metadata_str: dict[str, str]\n</code></pre> <p>The schema's metadata where keys and values are <code>str</code>, not <code>bytes</code>.</p>"},{"location":"api/core/field/#arro3.core.Field.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>The field name.</p>"},{"location":"api/core/field/#arro3.core.Field.nullable","title":"nullable  <code>property</code>","text":"<pre><code>nullable: bool\n</code></pre> <p>The field nullability.</p>"},{"location":"api/core/field/#arro3.core.Field.type","title":"type  <code>property</code>","text":"<pre><code>type: DataType\n</code></pre> <p>Access the data type of this field.</p>"},{"location":"api/core/field/#arro3.core.Field.__arrow_c_schema__","title":"__arrow_c_schema__","text":"<pre><code>__arrow_c_schema__() -&gt; object\n</code></pre> <p>An implementation of the Arrow PyCapsule Interface. This dunder method should not be called directly, but enables zero-copy data transfer to other Python libraries that understand Arrow memory.</p> <p>For example, you can call <code>pyarrow.field()</code> to convert this array into a pyarrow field, without copying memory.</p>"},{"location":"api/core/field/#arro3.core.Field.equals","title":"equals","text":"<pre><code>equals(other: ArrowSchemaExportable) -&gt; bool\n</code></pre> <p>Test if this field is equal to the other.</p>"},{"location":"api/core/field/#arro3.core.Field.from_arrow","title":"from_arrow  <code>classmethod</code>","text":"<pre><code>from_arrow(input: ArrowSchemaExportable) -&gt; Field\n</code></pre> <p>Construct this from an existing Arrow object.</p> <p>It can be called on anything that exports the Arrow schema interface (has an <code>__arrow_c_schema__</code> method).</p>"},{"location":"api/core/field/#arro3.core.Field.from_arrow_pycapsule","title":"from_arrow_pycapsule  <code>classmethod</code>","text":"<pre><code>from_arrow_pycapsule(capsule) -&gt; Field\n</code></pre> <p>Construct this object from a bare Arrow PyCapsule</p>"},{"location":"api/core/field/#arro3.core.Field.remove_metadata","title":"remove_metadata","text":"<pre><code>remove_metadata() -&gt; Field\n</code></pre> <p>Create new field without metadata, if any.</p>"},{"location":"api/core/field/#arro3.core.Field.with_metadata","title":"with_metadata","text":"<pre><code>with_metadata(metadata: dict[str, str] | dict[bytes, bytes]) -&gt; Field\n</code></pre> <p>Add metadata as dict of string keys and values to Field.</p>"},{"location":"api/core/field/#arro3.core.Field.with_name","title":"with_name","text":"<pre><code>with_name(name: str) -&gt; Field\n</code></pre> <p>A copy of this field with the replaced name.</p>"},{"location":"api/core/field/#arro3.core.Field.with_nullable","title":"with_nullable","text":"<pre><code>with_nullable(nullable: bool) -&gt; Field\n</code></pre> <p>A copy of this field with the replaced nullability.</p>"},{"location":"api/core/field/#arro3.core.Field.with_type","title":"with_type","text":"<pre><code>with_type(new_type: ArrowSchemaExportable) -&gt; Field\n</code></pre> <p>A copy of this field with the replaced type</p>"},{"location":"api/core/record-batch-reader/","title":"RecordBatchReader","text":""},{"location":"api/core/record-batch-reader/#arro3.core.RecordBatchReader","title":"arro3.core.RecordBatchReader","text":"<p>An Arrow RecordBatchReader.</p> <p>A RecordBatchReader holds a stream of <code>RecordBatch</code>.</p>"},{"location":"api/core/record-batch-reader/#arro3.core.RecordBatchReader.closed","title":"closed  <code>property</code>","text":"<pre><code>closed: bool\n</code></pre> <p>Returns <code>true</code> if this reader has already been consumed.</p>"},{"location":"api/core/record-batch-reader/#arro3.core.RecordBatchReader.schema","title":"schema  <code>property</code>","text":"<pre><code>schema: Schema\n</code></pre> <p>Access the schema of this table.</p>"},{"location":"api/core/record-batch-reader/#arro3.core.RecordBatchReader.__arrow_c_schema__","title":"__arrow_c_schema__","text":"<pre><code>__arrow_c_schema__() -&gt; object\n</code></pre> <p>An implementation of the Arrow PyCapsule Interface. This dunder method should not be called directly, but enables zero-copy data transfer to other Python libraries that understand Arrow memory.</p> <p>This allows Arrow consumers to inspect the data type of this RecordBatchReader. Then the consumer can ask the producer (in <code>__arrow_c_stream__</code>) to cast the exported data to a supported data type.</p>"},{"location":"api/core/record-batch-reader/#arro3.core.RecordBatchReader.__arrow_c_stream__","title":"__arrow_c_stream__","text":"<pre><code>__arrow_c_stream__(requested_schema: object | None = None) -&gt; object\n</code></pre> <p>An implementation of the Arrow PyCapsule Interface. This dunder method should not be called directly, but enables zero-copy data transfer to other Python libraries that understand Arrow memory.</p> <p>For example, you can call <code>pyarrow.RecordBatchReader.from_stream</code> to convert this stream to a pyarrow <code>RecordBatchReader</code>. Alternatively, you can call <code>pyarrow.table()</code> to consume this stream to a pyarrow table or <code>Table.from_arrow()</code> to consume this stream to an arro3 Table.</p>"},{"location":"api/core/record-batch-reader/#arro3.core.RecordBatchReader.from_arrow","title":"from_arrow  <code>classmethod</code>","text":"<pre><code>from_arrow(\n    input: ArrowArrayExportable | ArrowStreamExportable,\n) -&gt; RecordBatchReader\n</code></pre> <p>Construct this from an existing Arrow object.</p> <p>It can be called on anything that exports the Arrow stream interface (has an <code>__arrow_c_stream__</code> method), such as a <code>Table</code> or <code>RecordBatchReader</code>.</p>"},{"location":"api/core/record-batch-reader/#arro3.core.RecordBatchReader.from_arrow_pycapsule","title":"from_arrow_pycapsule  <code>classmethod</code>","text":"<pre><code>from_arrow_pycapsule(capsule) -&gt; RecordBatchReader\n</code></pre> <p>Construct this object from a bare Arrow PyCapsule</p>"},{"location":"api/core/record-batch-reader/#arro3.core.RecordBatchReader.from_batches","title":"from_batches  <code>classmethod</code>","text":"<pre><code>from_batches(\n    schema: ArrowSchemaExportable, batches: Sequence[ArrowArrayExportable]\n) -&gt; RecordBatchReader\n</code></pre> <p>Construct a new RecordBatchReader from existing data.</p> <p>Parameters:</p> <ul> <li> <code>schema</code>               (<code>ArrowSchemaExportable</code>)           \u2013            <p>The schema of the Arrow batches.</p> </li> <li> <code>batches</code>               (<code>Sequence[ArrowArrayExportable]</code>)           \u2013            <p>The existing batches.</p> </li> </ul>"},{"location":"api/core/record-batch-reader/#arro3.core.RecordBatchReader.from_stream","title":"from_stream  <code>classmethod</code>","text":"<pre><code>from_stream(data: ArrowStreamExportable) -&gt; RecordBatchReader\n</code></pre> <p>Import a RecordBatchReader from an object that exports an Arrow C Stream.</p>"},{"location":"api/core/record-batch-reader/#arro3.core.RecordBatchReader.read_all","title":"read_all","text":"<pre><code>read_all() -&gt; Table\n</code></pre> <p>Read all batches into a Table.</p>"},{"location":"api/core/record-batch-reader/#arro3.core.RecordBatchReader.read_next_batch","title":"read_next_batch","text":"<pre><code>read_next_batch() -&gt; RecordBatch\n</code></pre> <p>Read the next batch in the stream.</p>"},{"location":"api/core/record-batch/","title":"RecordBatch","text":""},{"location":"api/core/record-batch/#arro3.core.RecordBatch","title":"arro3.core.RecordBatch","text":"<p>A two-dimensional batch of column-oriented data with a defined schema.</p> <p>A <code>RecordBatch</code> is a two-dimensional dataset of a number of contiguous arrays, each the same length. A record batch has a schema which must match its arrays' datatypes.</p> <p>Record batches are a convenient unit of work for various serialization and computation functions, possibly incremental.</p>"},{"location":"api/core/record-batch/#arro3.core.RecordBatch.column_names","title":"column_names  <code>property</code>","text":"<pre><code>column_names: list[str]\n</code></pre> <p>Names of the RecordBatch columns.</p>"},{"location":"api/core/record-batch/#arro3.core.RecordBatch.columns","title":"columns  <code>property</code>","text":"<pre><code>columns: list[Array]\n</code></pre> <p>List of all columns in numerical order.</p>"},{"location":"api/core/record-batch/#arro3.core.RecordBatch.nbytes","title":"nbytes  <code>property</code>","text":"<pre><code>nbytes: int\n</code></pre> <p>Total number of bytes consumed by the elements of the record batch.</p>"},{"location":"api/core/record-batch/#arro3.core.RecordBatch.num_columns","title":"num_columns  <code>property</code>","text":"<pre><code>num_columns: int\n</code></pre> <p>Number of columns.</p>"},{"location":"api/core/record-batch/#arro3.core.RecordBatch.num_rows","title":"num_rows  <code>property</code>","text":"<pre><code>num_rows: int\n</code></pre> <p>Number of rows</p> <p>Due to the definition of a RecordBatch, all columns have the same number of rows.</p>"},{"location":"api/core/record-batch/#arro3.core.RecordBatch.schema","title":"schema  <code>property</code>","text":"<pre><code>schema: Schema\n</code></pre> <p>Access the schema of this RecordBatch</p>"},{"location":"api/core/record-batch/#arro3.core.RecordBatch.shape","title":"shape  <code>property</code>","text":"<pre><code>shape: tuple[int, int]\n</code></pre> <p>Dimensions of the table or record batch: (number of rows, number of columns).</p>"},{"location":"api/core/record-batch/#arro3.core.RecordBatch.__arrow_c_array__","title":"__arrow_c_array__","text":"<pre><code>__arrow_c_array__(\n    requested_schema: object | None = None,\n) -&gt; tuple[object, object]\n</code></pre> <p>An implementation of the Arrow PyCapsule Interface. This dunder method should not be called directly, but enables zero-copy data transfer to other Python libraries that understand Arrow memory.</p> <p>For example, you can call <code>pyarrow.record_batch()</code> to convert this RecordBatch into a pyarrow RecordBatch, without copying memory.</p>"},{"location":"api/core/record-batch/#arro3.core.RecordBatch.__arrow_c_schema__","title":"__arrow_c_schema__","text":"<pre><code>__arrow_c_schema__() -&gt; object\n</code></pre> <p>An implementation of the Arrow PyCapsule Interface. This dunder method should not be called directly, but enables zero-copy data transfer to other Python libraries that understand Arrow memory.</p> <p>This allows Arrow consumers to inspect the data type of this RecordBatch. Then the consumer can ask the producer (in <code>__arrow_c_array__</code>) to cast the exported data to a supported data type.</p>"},{"location":"api/core/record-batch/#arro3.core.RecordBatch.add_column","title":"add_column","text":"<pre><code>add_column(\n    i: int, field: str | ArrowSchemaExportable, column: ArrayInput\n) -&gt; RecordBatch\n</code></pre> <p>Add column to RecordBatch at position.</p> <p>A new RecordBatch is returned with the column added, the original RecordBatch object is left unchanged.</p> <p>Parameters:</p> <ul> <li> <code>i</code>               (<code>int</code>)           \u2013            <p>Index to place the column at.</p> </li> <li> <code>field</code>               (<code>str | ArrowSchemaExportable</code>)           \u2013            <p>description</p> </li> <li> <code>column</code>               (<code>ArrayInput</code>)           \u2013            <p>Column data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>RecordBatch</code>           \u2013            <p>New RecordBatch with the passed column added.</p> </li> </ul>"},{"location":"api/core/record-batch/#arro3.core.RecordBatch.append_column","title":"append_column","text":"<pre><code>append_column(\n    field: str | ArrowSchemaExportable, column: ArrayInput\n) -&gt; RecordBatch\n</code></pre> <p>Append column at end of columns.</p> <p>Parameters:</p> <ul> <li> <code>field</code>               (<code>str | ArrowSchemaExportable</code>)           \u2013            <p>If a string is passed then the type is deduced from the column data.</p> </li> <li> <code>column</code>               (<code>ArrayInput</code>)           \u2013            <p>Column data</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>RecordBatch</code>           \u2013            <p>description</p> </li> </ul>"},{"location":"api/core/record-batch/#arro3.core.RecordBatch.column","title":"column","text":"<pre><code>column(i: int | str) -&gt; Array\n</code></pre> <p>Select single column from Table or RecordBatch.</p> <p>Parameters:</p> <ul> <li> <code>i</code>               (<code>int | str</code>)           \u2013            <p>The index or name of the column to retrieve.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Array</code>           \u2013            <p>description</p> </li> </ul>"},{"location":"api/core/record-batch/#arro3.core.RecordBatch.equals","title":"equals","text":"<pre><code>equals(other: ArrowArrayExportable) -&gt; bool\n</code></pre> <p>Check if contents of two record batches are equal.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>ArrowArrayExportable</code>)           \u2013            <p>RecordBatch to compare against.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>description</p> </li> </ul>"},{"location":"api/core/record-batch/#arro3.core.RecordBatch.field","title":"field","text":"<pre><code>field(i: int | str) -&gt; Field\n</code></pre> <p>Select a schema field by its column name or numeric index.</p> <p>Parameters:</p> <ul> <li> <code>i</code>               (<code>int | str</code>)           \u2013            <p>The index or name of the field to retrieve.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Field</code>           \u2013            <p>description</p> </li> </ul>"},{"location":"api/core/record-batch/#arro3.core.RecordBatch.from_arrays","title":"from_arrays  <code>classmethod</code>","text":"<pre><code>from_arrays(\n    arrays: Sequence[ArrayInput], *, schema: ArrowSchemaExportable\n) -&gt; RecordBatch\n</code></pre> <p>Construct a RecordBatch from multiple Arrays</p> <p>Parameters:</p> <ul> <li> <code>arrays</code>               (<code>Sequence[ArrayInput]</code>)           \u2013            <p>One for each field in RecordBatch</p> </li> <li> <code>schema</code>               (<code>ArrowSchemaExportable</code>)           \u2013            <p>Schema for the created batch. If not passed, names must be passed</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>RecordBatch</code>           \u2013            <p>description</p> </li> </ul>"},{"location":"api/core/record-batch/#arro3.core.RecordBatch.from_arrow","title":"from_arrow  <code>classmethod</code>","text":"<pre><code>from_arrow(input: ArrowArrayExportable | ArrowStreamExportable) -&gt; RecordBatch\n</code></pre> <p>Construct this from an existing Arrow RecordBatch.</p> <p>It can be called on anything that exports the Arrow data interface (has a <code>__arrow_c_array__</code> method) and returns a StructArray..</p> <p>Parameters:</p> <ul> <li> <code>input</code>               (<code>ArrowArrayExportable | ArrowStreamExportable</code>)           \u2013            <p>Arrow array to use for constructing this object</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>RecordBatch</code>           \u2013            <p>new RecordBatch</p> </li> </ul>"},{"location":"api/core/record-batch/#arro3.core.RecordBatch.from_arrow_pycapsule","title":"from_arrow_pycapsule  <code>classmethod</code>","text":"<pre><code>from_arrow_pycapsule(schema_capsule, array_capsule) -&gt; RecordBatch\n</code></pre> <p>Construct this object from bare Arrow PyCapsules</p>"},{"location":"api/core/record-batch/#arro3.core.RecordBatch.from_pydict","title":"from_pydict  <code>classmethod</code>","text":"<pre><code>from_pydict(\n    mapping: dict[str, ArrayInput],\n    *,\n    metadata: dict[str, str] | dict[bytes, bytes] | None = None\n) -&gt; RecordBatch\n</code></pre> <p>Construct a Table or RecordBatch from Arrow arrays or columns.</p> <p>Parameters:</p> <ul> <li> <code>mapping</code>               (<code>dict[str, ArrayInput]</code>)           \u2013            <p>A mapping of strings to Arrays.</p> </li> <li> <code>metadata</code>               (<code>dict[str, str] | dict[bytes, bytes] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional metadata for the schema (if inferred). Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>RecordBatch</code>           \u2013            <p>description</p> </li> </ul>"},{"location":"api/core/record-batch/#arro3.core.RecordBatch.from_struct_array","title":"from_struct_array  <code>classmethod</code>","text":"<pre><code>from_struct_array(struct_array: ArrowArrayExportable) -&gt; RecordBatch\n</code></pre> <p>Construct a RecordBatch from a StructArray.</p> <p>Each field in the StructArray will become a column in the resulting RecordBatch.</p> <p>Parameters:</p> <ul> <li> <code>struct_array</code>               (<code>ArrowArrayExportable</code>)           \u2013            <p>Array to construct the record batch from.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>RecordBatch</code>           \u2013            <p>New RecordBatch</p> </li> </ul>"},{"location":"api/core/record-batch/#arro3.core.RecordBatch.remove_column","title":"remove_column","text":"<pre><code>remove_column(i: int) -&gt; RecordBatch\n</code></pre> <p>Create new RecordBatch with the indicated column removed.</p> <p>Parameters:</p> <ul> <li> <code>i</code>               (<code>int</code>)           \u2013            <p>Index of column to remove.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>RecordBatch</code>           \u2013            <p>New record batch without the column.</p> </li> </ul>"},{"location":"api/core/record-batch/#arro3.core.RecordBatch.select","title":"select","text":"<pre><code>select(columns: list[int] | list[str]) -&gt; RecordBatch\n</code></pre> <p>Select columns of the RecordBatch.</p> <p>Returns a new RecordBatch with the specified columns, and metadata preserved.</p> <p>Parameters:</p> <ul> <li> <code>columns</code>               (<code>list[int] | list[str]</code>)           \u2013            <p>The column names or integer indices to select.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>RecordBatch</code>           \u2013            <p>New RecordBatch.</p> </li> </ul>"},{"location":"api/core/record-batch/#arro3.core.RecordBatch.set_column","title":"set_column","text":"<pre><code>set_column(\n    i: int, field: str | ArrowSchemaExportable, column: ArrayInput\n) -&gt; RecordBatch\n</code></pre> <p>Replace column in RecordBatch at position.</p> <p>Parameters:</p> <ul> <li> <code>i</code>               (<code>int</code>)           \u2013            <p>Index to place the column at.</p> </li> <li> <code>field</code>               (<code>str | ArrowSchemaExportable</code>)           \u2013            <p>If a string is passed then the type is deduced from the column data.</p> </li> <li> <code>column</code>               (<code>ArrayInput</code>)           \u2013            <p>Column data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>RecordBatch</code>           \u2013            <p>New RecordBatch.</p> </li> </ul>"},{"location":"api/core/record-batch/#arro3.core.RecordBatch.slice","title":"slice","text":"<pre><code>slice(offset: int = 0, length: int | None = None) -&gt; RecordBatch\n</code></pre> <p>Compute zero-copy slice of this RecordBatch</p> <p>Parameters:</p> <ul> <li> <code>offset</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Offset from start of record batch to slice. Defaults to 0.</p> </li> <li> <code>length</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Length of slice (default is until end of batch starting from offset). Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>RecordBatch</code>           \u2013            <p>New RecordBatch.</p> </li> </ul>"},{"location":"api/core/record-batch/#arro3.core.RecordBatch.take","title":"take","text":"<pre><code>take(indices: ArrayInput) -&gt; RecordBatch\n</code></pre> <p>Select rows from a Table or RecordBatch.</p> <p>Parameters:</p> <ul> <li> <code>indices</code>               (<code>ArrayInput</code>)           \u2013            <p>The indices in the tabular object whose rows will be returned.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>RecordBatch</code>           \u2013            <p>description</p> </li> </ul>"},{"location":"api/core/record-batch/#arro3.core.RecordBatch.to_struct_array","title":"to_struct_array","text":"<pre><code>to_struct_array() -&gt; Array\n</code></pre> <p>Convert to a struct array.</p> <p>Returns:</p> <ul> <li> <code>Array</code>           \u2013            <p>description</p> </li> </ul>"},{"location":"api/core/record-batch/#arro3.core.RecordBatch.with_schema","title":"with_schema","text":"<pre><code>with_schema(schema: ArrowSchemaExportable) -&gt; RecordBatch\n</code></pre> <p>Return a RecordBatch with the provided schema.</p>"},{"location":"api/core/scalar/","title":"Scalar","text":""},{"location":"api/core/scalar/#arro3.core.Scalar","title":"arro3.core.Scalar","text":"<p>An arrow Scalar.</p>"},{"location":"api/core/scalar/#arro3.core.Scalar.field","title":"field  <code>property</code>","text":"<pre><code>field: Field\n</code></pre> <p>Access the field stored on this Scalar.</p> <p>Note that this field usually will not have a name associated, but it may have metadata that signifies that this scalar is an extension (user-defined typed) scalar.</p>"},{"location":"api/core/scalar/#arro3.core.Scalar.is_valid","title":"is_valid  <code>property</code>","text":"<pre><code>is_valid: bool\n</code></pre> <p>Return <code>True</code> if this scalar is not null.</p>"},{"location":"api/core/scalar/#arro3.core.Scalar.type","title":"type  <code>property</code>","text":"<pre><code>type: DataType\n</code></pre> <p>Access the type of this scalar.</p>"},{"location":"api/core/scalar/#arro3.core.Scalar.__arrow_c_array__","title":"__arrow_c_array__","text":"<pre><code>__arrow_c_array__(\n    requested_schema: object | None = None,\n) -&gt; tuple[object, object]\n</code></pre> <p>An implementation of the Arrow PyCapsule Interface. This dunder method should not be called directly, but enables zero-copy data transfer to other Python libraries that understand Arrow memory.</p> <p>For example, you can call <code>pyarrow.array()</code> to convert this Scalar into a pyarrow Array, without copying memory. The generated array is guaranteed to have length 1.</p>"},{"location":"api/core/scalar/#arro3.core.Scalar.as_py","title":"as_py","text":"<pre><code>as_py() -&gt; Any\n</code></pre> <p>Convert this scalar to a pure-Python object.</p>"},{"location":"api/core/scalar/#arro3.core.Scalar.cast","title":"cast","text":"<pre><code>cast(target_type: ArrowSchemaExportable) -&gt; Scalar\n</code></pre> <p>Cast scalar to another data type</p> <p>Parameters:</p> <ul> <li> <code>target_type</code>               (<code>ArrowSchemaExportable</code>)           \u2013            <p>Type to cast to.</p> </li> </ul>"},{"location":"api/core/scalar/#arro3.core.Scalar.from_arrow","title":"from_arrow  <code>classmethod</code>","text":"<pre><code>from_arrow(input: ArrowArrayExportable) -&gt; Scalar\n</code></pre> <p>Construct this from an existing Arrow Scalar.</p> <p>It can be called on anything that exports the Arrow data interface (has a <code>__arrow_c_array__</code> method) and returns an array with a single element.</p> <p>Parameters:</p> <ul> <li> <code>input</code>               (<code>ArrowArrayExportable</code>)           \u2013            <p>Arrow scalar to use for constructing this object</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Scalar</code>           \u2013            <p>new Scalar</p> </li> </ul>"},{"location":"api/core/scalar/#arro3.core.Scalar.from_arrow_pycapsule","title":"from_arrow_pycapsule  <code>classmethod</code>","text":"<pre><code>from_arrow_pycapsule(schema_capsule, array_capsule) -&gt; Scalar\n</code></pre> <p>Construct this object from bare Arrow PyCapsules</p>"},{"location":"api/core/schema/","title":"Schema","text":""},{"location":"api/core/schema/#arro3.core.Schema","title":"arro3.core.Schema","text":"<p>An arrow Schema.</p>"},{"location":"api/core/schema/#arro3.core.Schema.metadata","title":"metadata  <code>property</code>","text":"<pre><code>metadata: dict[bytes, bytes]\n</code></pre> <p>The schema's metadata.</p> <p>Returns:</p> <ul> <li> <code>dict[bytes, bytes]</code>           \u2013            <p>description</p> </li> </ul>"},{"location":"api/core/schema/#arro3.core.Schema.metadata_str","title":"metadata_str  <code>property</code>","text":"<pre><code>metadata_str: dict[str, str]\n</code></pre> <p>The schema's metadata where keys and values are <code>str</code>, not <code>bytes</code>.</p> <p>Returns:</p> <ul> <li> <code>dict[str, str]</code>           \u2013            <p>description</p> </li> </ul>"},{"location":"api/core/schema/#arro3.core.Schema.names","title":"names  <code>property</code>","text":"<pre><code>names: list[str]\n</code></pre> <p>The schema's field names.</p>"},{"location":"api/core/schema/#arro3.core.Schema.types","title":"types  <code>property</code>","text":"<pre><code>types: list[DataType]\n</code></pre> <p>The schema's field types.</p> <p>Returns:</p> <ul> <li> <code>list[DataType]</code>           \u2013            <p>description</p> </li> </ul>"},{"location":"api/core/schema/#arro3.core.Schema.__arrow_c_schema__","title":"__arrow_c_schema__","text":"<pre><code>__arrow_c_schema__() -&gt; object\n</code></pre> <p>An implementation of the Arrow PyCapsule Interface. This dunder method should not be called directly, but enables zero-copy data transfer to other Python libraries that understand Arrow memory.</p> <p>For example, you can call <code>pyarrow.schema()</code> to convert this array into a pyarrow schema, without copying memory.</p>"},{"location":"api/core/schema/#arro3.core.Schema.append","title":"append","text":"<pre><code>append(field: ArrowSchemaExportable) -&gt; Schema\n</code></pre> <p>Append a field at the end of the schema.</p> <p>In contrast to Python's <code>list.append()</code> it does return a new object, leaving the original Schema unmodified.</p> <p>Parameters:</p> <ul> <li> <code>field</code>               (<code>ArrowSchemaExportable</code>)           \u2013            <p>new field</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Schema</code>           \u2013            <p>New Schema</p> </li> </ul>"},{"location":"api/core/schema/#arro3.core.Schema.empty_table","title":"empty_table","text":"<pre><code>empty_table() -&gt; Table\n</code></pre> <p>Provide an empty table according to the schema.</p> <p>Returns:</p> <ul> <li> <code>Table</code>           \u2013            <p>Table</p> </li> </ul>"},{"location":"api/core/schema/#arro3.core.Schema.equals","title":"equals","text":"<pre><code>equals(other: ArrowSchemaExportable) -&gt; bool\n</code></pre> <p>Test if this schema is equal to the other</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>ArrowSchemaExportable</code>)           \u2013            <p>description</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>description</p> </li> </ul>"},{"location":"api/core/schema/#arro3.core.Schema.field","title":"field","text":"<pre><code>field(i: int | str) -&gt; Field\n</code></pre> <p>Select a field by its column name or numeric index.</p> <p>Parameters:</p> <ul> <li> <code>i</code>               (<code>int | str</code>)           \u2013            <p>other</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Field</code>           \u2013            <p>description</p> </li> </ul>"},{"location":"api/core/schema/#arro3.core.Schema.from_arrow","title":"from_arrow  <code>classmethod</code>","text":"<pre><code>from_arrow(input: ArrowSchemaExportable) -&gt; Schema\n</code></pre> <p>Construct this from an existing Arrow object</p> <p>Parameters:</p> <ul> <li> <code>input</code>               (<code>ArrowSchemaExportable</code>)           \u2013            <p>Arrow schema to use for constructing this object</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Schema</code>           \u2013            <p>description</p> </li> </ul>"},{"location":"api/core/schema/#arro3.core.Schema.from_arrow_pycapsule","title":"from_arrow_pycapsule  <code>classmethod</code>","text":"<pre><code>from_arrow_pycapsule(capsule) -&gt; Schema\n</code></pre> <p>Construct this object from a bare Arrow PyCapsule</p>"},{"location":"api/core/schema/#arro3.core.Schema.get_all_field_indices","title":"get_all_field_indices","text":"<pre><code>get_all_field_indices(name: str) -&gt; list[int]\n</code></pre> <p>Return sorted list of indices for the fields with the given name.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>description</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[int]</code>           \u2013            <p>description</p> </li> </ul>"},{"location":"api/core/schema/#arro3.core.Schema.get_field_index","title":"get_field_index","text":"<pre><code>get_field_index(name: str) -&gt; int\n</code></pre> <p>Return index of the unique field with the given name.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>description</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>int</code>           \u2013            <p>description</p> </li> </ul>"},{"location":"api/core/schema/#arro3.core.Schema.insert","title":"insert","text":"<pre><code>insert(i: int, field: ArrowSchemaExportable) -&gt; Schema\n</code></pre> <p>Add a field at position <code>i</code> to the schema.</p> <p>Parameters:</p> <ul> <li> <code>i</code>               (<code>int</code>)           \u2013            <p>description</p> </li> <li> <code>field</code>               (<code>ArrowSchemaExportable</code>)           \u2013            <p>description</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Schema</code>           \u2013            <p>description</p> </li> </ul>"},{"location":"api/core/schema/#arro3.core.Schema.remove","title":"remove","text":"<pre><code>remove(i: int) -&gt; Schema\n</code></pre> <p>Remove the field at index i from the schema.</p> <p>Parameters:</p> <ul> <li> <code>i</code>               (<code>int</code>)           \u2013            <p>description</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Schema</code>           \u2013            <p>description</p> </li> </ul>"},{"location":"api/core/schema/#arro3.core.Schema.remove_metadata","title":"remove_metadata","text":"<pre><code>remove_metadata() -&gt; Schema\n</code></pre> <p>Create new schema without metadata, if any</p> <p>Returns:</p> <ul> <li> <code>Schema</code>           \u2013            <p>description</p> </li> </ul>"},{"location":"api/core/schema/#arro3.core.Schema.set","title":"set","text":"<pre><code>set(i: int, field: ArrowSchemaExportable) -&gt; Schema\n</code></pre> <p>Replace a field at position <code>i</code> in the schema.</p> <p>Parameters:</p> <ul> <li> <code>i</code>               (<code>int</code>)           \u2013            <p>description</p> </li> <li> <code>field</code>               (<code>ArrowSchemaExportable</code>)           \u2013            <p>description</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Schema</code>           \u2013            <p>description</p> </li> </ul>"},{"location":"api/core/schema/#arro3.core.Schema.with_metadata","title":"with_metadata","text":"<pre><code>with_metadata(metadata: dict[str, str] | dict[bytes, bytes]) -&gt; Schema\n</code></pre> <p>Add metadata as dict of string keys and values to Schema.</p> <p>Parameters:</p> <ul> <li> <code>metadata</code>               (<code>dict[str, str] | dict[bytes, bytes]</code>)           \u2013            <p>description</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Schema</code>           \u2013            <p>description</p> </li> </ul>"},{"location":"api/core/table/","title":"Table","text":""},{"location":"api/core/table/#arro3.core.Table","title":"arro3.core.Table","text":"<p>A collection of top-level named, equal length Arrow arrays.</p>"},{"location":"api/core/table/#arro3.core.Table.chunk_lengths","title":"chunk_lengths  <code>property</code>","text":"<pre><code>chunk_lengths: list[int]\n</code></pre> <p>The number of rows in each internal chunk.</p>"},{"location":"api/core/table/#arro3.core.Table.column_names","title":"column_names  <code>property</code>","text":"<pre><code>column_names: list[str]\n</code></pre> <p>Names of the Table or RecordBatch columns.</p> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>description</p> </li> </ul>"},{"location":"api/core/table/#arro3.core.Table.columns","title":"columns  <code>property</code>","text":"<pre><code>columns: list[ChunkedArray]\n</code></pre> <p>List of all columns in numerical order.</p> <p>Returns:</p> <ul> <li> <code>list[ChunkedArray]</code>           \u2013            <p>description</p> </li> </ul>"},{"location":"api/core/table/#arro3.core.Table.nbytes","title":"nbytes  <code>property</code>","text":"<pre><code>nbytes: int\n</code></pre> <p>Total number of bytes consumed by the elements of the table.</p>"},{"location":"api/core/table/#arro3.core.Table.num_columns","title":"num_columns  <code>property</code>","text":"<pre><code>num_columns: int\n</code></pre> <p>Number of columns in this table.</p>"},{"location":"api/core/table/#arro3.core.Table.num_rows","title":"num_rows  <code>property</code>","text":"<pre><code>num_rows: int\n</code></pre> <p>Number of rows in this table.</p> <p>Due to the definition of a table, all columns have the same number of rows.</p>"},{"location":"api/core/table/#arro3.core.Table.schema","title":"schema  <code>property</code>","text":"<pre><code>schema: Schema\n</code></pre> <p>Schema of the table and its columns.</p> <p>Returns:</p> <ul> <li> <code>Schema</code>           \u2013            <p>description</p> </li> </ul>"},{"location":"api/core/table/#arro3.core.Table.shape","title":"shape  <code>property</code>","text":"<pre><code>shape: tuple[int, int]\n</code></pre> <p>Dimensions of the table or record batch</p> <p>Returns:</p> <ul> <li> <code>tuple[int, int]</code>           \u2013            <p>(number of rows, number of columns)</p> </li> </ul>"},{"location":"api/core/table/#arro3.core.Table.__arrow_c_schema__","title":"__arrow_c_schema__","text":"<pre><code>__arrow_c_schema__() -&gt; object\n</code></pre> <p>An implementation of the Arrow PyCapsule Interface. This dunder method should not be called directly, but enables zero-copy data transfer to other Python libraries that understand Arrow memory.</p> <p>This allows Arrow consumers to inspect the data type of this Table. Then the consumer can ask the producer (in <code>__arrow_c_stream__</code>) to cast the exported data to a supported data type.</p>"},{"location":"api/core/table/#arro3.core.Table.__arrow_c_stream__","title":"__arrow_c_stream__","text":"<pre><code>__arrow_c_stream__(requested_schema: object | None = None) -&gt; object\n</code></pre> <p>An implementation of the Arrow PyCapsule Interface. This dunder method should not be called directly, but enables zero-copy data transfer to other Python libraries that understand Arrow memory.</p> <p>For example, you can call <code>pyarrow.table()</code> to convert this array into a pyarrow table, without copying memory.</p>"},{"location":"api/core/table/#arro3.core.Table.add_column","title":"add_column","text":"<pre><code>add_column(\n    i: int, field: str | ArrowSchemaExportable, column: ArrowStreamExportable\n) -&gt; Table\n</code></pre> <p>Add column to Table at position.</p> <p>A new table is returned with the column added, the original table object is left unchanged.</p> <p>Parameters:</p> <ul> <li> <code>i</code>               (<code>int</code>)           \u2013            <p>Index to place the column at.</p> </li> <li> <code>field</code>               (<code>str | ArrowSchemaExportable</code>)           \u2013            <p>description</p> </li> <li> <code>column</code>               (<code>ArrowStreamExportable</code>)           \u2013            <p>Column data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Table</code>           \u2013            <p>New table with the passed column added.</p> </li> </ul>"},{"location":"api/core/table/#arro3.core.Table.append_column","title":"append_column","text":"<pre><code>append_column(\n    field: str | ArrowSchemaExportable, column: ArrowStreamExportable\n) -&gt; Table\n</code></pre> <p>Append column at end of columns.</p> <p>Parameters:</p> <ul> <li> <code>field</code>               (<code>str | ArrowSchemaExportable</code>)           \u2013            <p>description</p> </li> <li> <code>column</code>               (<code>ArrowStreamExportable</code>)           \u2013            <p>Column data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Table</code>           \u2013            <p>New table or record batch with the passed column added.</p> </li> </ul>"},{"location":"api/core/table/#arro3.core.Table.column","title":"column","text":"<pre><code>column(i: int | str) -&gt; ChunkedArray\n</code></pre> <p>Select single column from Table or RecordBatch.</p> <p>Parameters:</p> <ul> <li> <code>i</code>               (<code>int | str</code>)           \u2013            <p>The index or name of the column to retrieve.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ChunkedArray</code>           \u2013            <p>description</p> </li> </ul>"},{"location":"api/core/table/#arro3.core.Table.combine_chunks","title":"combine_chunks","text":"<pre><code>combine_chunks() -&gt; Table\n</code></pre> <p>Make a new table by combining the chunks this table has.</p> <p>All the underlying chunks in the ChunkedArray of each column are concatenated into zero or one chunk.</p> <p>Returns:</p> <ul> <li> <code>Table</code>           \u2013            <p>new Table with one or zero chunks.</p> </li> </ul>"},{"location":"api/core/table/#arro3.core.Table.field","title":"field","text":"<pre><code>field(i: int | str) -&gt; Field\n</code></pre> <p>Select a schema field by its column name or numeric index.</p> <p>Parameters:</p> <ul> <li> <code>i</code>               (<code>int | str</code>)           \u2013            <p>The index or name of the field to retrieve.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Field</code>           \u2013            <p>description</p> </li> </ul>"},{"location":"api/core/table/#arro3.core.Table.from_arrays","title":"from_arrays  <code>classmethod</code>","text":"<pre><code>from_arrays(\n    arrays: Sequence[ArrayInput | ArrowStreamExportable],\n    *,\n    names: Sequence[str] | None = None,\n    schema: ArrowSchemaExportable | None = None,\n    metadata: dict[str, str] | dict[bytes, bytes] | None = None\n) -&gt; Table\n</code></pre> <p>Construct a Table from Arrow arrays.</p> <p>Parameters:</p> <ul> <li> <code>arrays</code>               (<code>Sequence[ArrayInput | ArrowStreamExportable]</code>)           \u2013            <p>Equal-length arrays that should form the table.</p> </li> <li> <code>names</code>               (<code>Sequence[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Names for the table columns. If not passed, <code>schema</code> must be passed. Defaults to None.</p> </li> <li> <code>schema</code>               (<code>ArrowSchemaExportable | None</code>, default:                   <code>None</code> )           \u2013            <p>Schema for the created table. If not passed, <code>names</code> must be passed. Defaults to None.</p> </li> <li> <code>metadata</code>               (<code>dict[str, str] | dict[bytes, bytes] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional metadata for the schema (if inferred). Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Table</code>           \u2013            <p>new table</p> </li> </ul>"},{"location":"api/core/table/#arro3.core.Table.from_arrow","title":"from_arrow  <code>classmethod</code>","text":"<pre><code>from_arrow(input: ArrowArrayExportable | ArrowStreamExportable) -&gt; Table\n</code></pre> <p>Construct this object from an existing Arrow object.</p> <p>It can be called on anything that exports the Arrow stream interface (<code>__arrow_c_stream__</code>) and yields a StructArray for each item. This Table will materialize all items from the iterator in memory at once. Use [<code>RecordBatchReader</code>] if you don't wish to materialize all batches in memory at once.</p> <p>Parameters:</p> <ul> <li> <code>input</code>               (<code>ArrowArrayExportable | ArrowStreamExportable</code>)           \u2013            <p>Arrow stream to use for constructing this object</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Table</code>           \u2013            <p>Self</p> </li> </ul>"},{"location":"api/core/table/#arro3.core.Table.from_arrow_pycapsule","title":"from_arrow_pycapsule  <code>classmethod</code>","text":"<pre><code>from_arrow_pycapsule(capsule) -&gt; Table\n</code></pre> <p>Construct this object from a bare Arrow PyCapsule</p> <p>Parameters:</p> <ul> <li> <code>capsule</code>           \u2013            <p>description</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Table</code>           \u2013            <p>description</p> </li> </ul>"},{"location":"api/core/table/#arro3.core.Table.from_batches","title":"from_batches  <code>classmethod</code>","text":"<pre><code>from_batches(\n    batches: Sequence[ArrowArrayExportable],\n    *,\n    schema: ArrowSchemaExportable | None = None\n) -&gt; Table\n</code></pre> <p>Construct a Table from a sequence of Arrow RecordBatches.</p> <p>Parameters:</p> <ul> <li> <code>batches</code>               (<code>Sequence[ArrowArrayExportable]</code>)           \u2013            <p>Sequence of RecordBatch to be converted, all schemas must be equal.</p> </li> <li> <code>schema</code>               (<code>ArrowSchemaExportable | None</code>, default:                   <code>None</code> )           \u2013            <p>If not passed, will be inferred from the first RecordBatch. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Table</code>           \u2013            <p>New Table.</p> </li> </ul>"},{"location":"api/core/table/#arro3.core.Table.from_pydict","title":"from_pydict  <code>classmethod</code>","text":"<pre><code>from_pydict(\n    mapping: dict[str, ArrayInput | ArrowStreamExportable],\n    *,\n    schema: ArrowSchemaExportable | None = None,\n    metadata: dict[str, str] | dict[bytes, bytes] | None = None\n) -&gt; Table\n</code></pre> <p>Construct a Table or RecordBatch from Arrow arrays or columns.</p> <p>Parameters:</p> <ul> <li> <code>mapping</code>               (<code>dict[str, ArrayInput | ArrowStreamExportable]</code>)           \u2013            <p>A mapping of strings to Arrays.</p> </li> <li> <code>schema</code>               (<code>ArrowSchemaExportable | None</code>, default:                   <code>None</code> )           \u2013            <p>If not passed, will be inferred from the Mapping values. Defaults to None.</p> </li> <li> <code>metadata</code>               (<code>dict[str, str] | dict[bytes, bytes] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional metadata for the schema (if inferred). Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Table</code>           \u2013            <p>new table</p> </li> </ul>"},{"location":"api/core/table/#arro3.core.Table.rechunk","title":"rechunk","text":"<pre><code>rechunk(*, max_chunksize: int | None = None) -&gt; Table\n</code></pre> <p>Rechunk a table with a maximum number of rows per chunk.</p> <p>Parameters:</p> <ul> <li> <code>max_chunksize</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The maximum number of rows per internal RecordBatch. Defaults to None, which rechunks into a single batch.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Table</code>           \u2013            <p>The rechunked table.</p> </li> </ul>"},{"location":"api/core/table/#arro3.core.Table.remove_column","title":"remove_column","text":"<pre><code>remove_column(i: int) -&gt; Table\n</code></pre> <p>Create new Table with the indicated column removed.</p> <p>Parameters:</p> <ul> <li> <code>i</code>               (<code>int</code>)           \u2013            <p>Index of column to remove.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Table</code>           \u2013            <p>New table without the column.</p> </li> </ul>"},{"location":"api/core/table/#arro3.core.Table.rename_columns","title":"rename_columns","text":"<pre><code>rename_columns(names: Sequence[str]) -&gt; Table\n</code></pre> <p>Create new table with columns renamed to provided names.</p> <p>Parameters:</p> <ul> <li> <code>names</code>               (<code>Sequence[str]</code>)           \u2013            <p>List of new column names.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Table</code>           \u2013            <p>description</p> </li> </ul>"},{"location":"api/core/table/#arro3.core.Table.select","title":"select","text":"<pre><code>select(columns: Sequence[int] | Sequence[str]) -&gt; Table\n</code></pre> <p>Select columns of the Table.</p> <p>Returns a new Table with the specified columns, and metadata preserved.</p> <p>Parameters:</p> <ul> <li> <code>columns</code>               (<code>Sequence[int] | Sequence[str]</code>)           \u2013            <p>The column names or integer indices to select.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Table</code>           \u2013            <p>description</p> </li> </ul>"},{"location":"api/core/table/#arro3.core.Table.set_column","title":"set_column","text":"<pre><code>set_column(\n    i: int, field: str | ArrowSchemaExportable, column: ArrowStreamExportable\n) -&gt; Table\n</code></pre> <p>Replace column in Table at position.</p> <p>Parameters:</p> <ul> <li> <code>i</code>               (<code>int</code>)           \u2013            <p>Index to place the column at.</p> </li> <li> <code>field</code>               (<code>str | ArrowSchemaExportable</code>)           \u2013            <p>description</p> </li> <li> <code>column</code>               (<code>ArrowStreamExportable</code>)           \u2013            <p>Column data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Table</code>           \u2013            <p>description</p> </li> </ul>"},{"location":"api/core/table/#arro3.core.Table.slice","title":"slice","text":"<pre><code>slice(offset: int = 0, length: int | None = None) -&gt; Table\n</code></pre> <p>Compute zero-copy slice of this table.</p> <p>Parameters:</p> <ul> <li> <code>offset</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Defaults to 0.</p> </li> <li> <code>length</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Table</code>           \u2013            <p>The sliced table</p> </li> </ul>"},{"location":"api/core/table/#arro3.core.Table.to_batches","title":"to_batches","text":"<pre><code>to_batches() -&gt; list[RecordBatch]\n</code></pre> <p>Convert Table to a list of RecordBatch objects.</p> <p>Note that this method is zero-copy, it merely exposes the same data under a different API.</p> <p>Returns:</p> <ul> <li> <code>list[RecordBatch]</code>           \u2013            <p>description</p> </li> </ul>"},{"location":"api/core/table/#arro3.core.Table.to_reader","title":"to_reader","text":"<pre><code>to_reader() -&gt; RecordBatchReader\n</code></pre> <p>Convert the Table to a RecordBatchReader.</p> <p>Note that this method is zero-copy, it merely exposes the same data under a different API.</p> <p>Returns:</p> <ul> <li> <code>RecordBatchReader</code>           \u2013            <p>description</p> </li> </ul>"},{"location":"api/core/table/#arro3.core.Table.to_struct_array","title":"to_struct_array","text":"<pre><code>to_struct_array() -&gt; ChunkedArray\n</code></pre> <p>Convert to a chunked array of struct type.</p> <p>Returns:</p> <ul> <li> <code>ChunkedArray</code>           \u2013            <p>description</p> </li> </ul>"},{"location":"api/core/table/#arro3.core.Table.with_schema","title":"with_schema","text":"<pre><code>with_schema(schema: ArrowSchemaExportable) -&gt; Table\n</code></pre> <p>Assign a different schema onto this table.</p> <p>The new schema must be compatible with the existing data; this does not cast the underlying data to the new schema. This is primarily useful for changing the schema metadata.</p> <p>Parameters:</p> <ul> <li> <code>schema</code>               (<code>ArrowSchemaExportable</code>)           \u2013            <p>description</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Table</code>           \u2013            <p>description</p> </li> </ul>"},{"location":"api/core/types/","title":"types","text":""},{"location":"api/core/types/#arro3.core.types","title":"arro3.core.types","text":""},{"location":"api/core/types/#arro3.core.types.ArrayInput","title":"ArrayInput  <code>module-attribute</code>","text":"<pre><code>ArrayInput = Union[ArrowArrayExportable, BufferProtocolExportable, ndarray]\n</code></pre> <p>Accepted input as an Arrow array.</p> <p>Buffer protocol input (such as numpy arrays) will be interpreted zero-copy except in the case of boolean-typed input, which must be copied to the Arrow format.</p>"},{"location":"api/core/types/#arro3.core.types.ArrowArrayExportable","title":"ArrowArrayExportable","text":"<p>               Bases: <code>Protocol</code></p> <p>An object with an <code>__arrow_c_array__</code> method.</p> <p>Supported objects include:</p> <ul> <li>arro3 <code>Array</code> or <code>RecordBatch</code> objects.</li> <li>pyarrow <code>Array</code> or <code>RecordBatch</code> objects</li> </ul> <p>Such an object implements the Arrow C Data Interface interface via the Arrow PyCapsule Interface. This allows for zero-copy Arrow data interchange across libraries.</p>"},{"location":"api/core/types/#arro3.core.types.ArrowArrayExportable.__arrow_c_array__","title":"__arrow_c_array__","text":"<pre><code>__arrow_c_array__(\n    requested_schema: object | None = None,\n) -&gt; Tuple[object, object]\n</code></pre>"},{"location":"api/core/types/#arro3.core.types.ArrowSchemaExportable","title":"ArrowSchemaExportable","text":"<p>               Bases: <code>Protocol</code></p> <p>An object with an <code>__arrow_c_schema__</code> method.</p> <p>Supported objects include:</p> <ul> <li>arro3 <code>Schema</code>, <code>Field</code>, or <code>DataType</code> objects.</li> <li>pyarrow <code>Schema</code>, <code>Field</code>, or <code>DataType</code> objects.</li> </ul> <p>Such an object implements the Arrow C Data Interface interface via the Arrow PyCapsule Interface. This allows for zero-copy Arrow data interchange across libraries.</p>"},{"location":"api/core/types/#arro3.core.types.ArrowSchemaExportable.__arrow_c_schema__","title":"__arrow_c_schema__","text":"<pre><code>__arrow_c_schema__() -&gt; object\n</code></pre>"},{"location":"api/core/types/#arro3.core.types.ArrowStreamExportable","title":"ArrowStreamExportable","text":"<p>               Bases: <code>Protocol</code></p> <p>An object with an <code>__arrow_c_stream__</code> method.</p> <p>Supported objects include:</p> <ul> <li>arro3 <code>Table</code>, <code>RecordBatchReader</code>, <code>ChunkedArray</code>, or <code>ArrayReader</code> objects.</li> <li>Polars <code>Series</code> or <code>DataFrame</code> objects (polars v1.2 or higher)</li> <li>pyarrow <code>RecordBatchReader</code>, <code>Table</code>, or <code>ChunkedArray</code> objects (pyarrow v14 or     higher)</li> <li>pandas <code>DataFrame</code>s  (pandas v2.2 or higher)</li> <li>ibis <code>Table</code> objects.</li> </ul> <p>For an up to date list of supported objects, see this issue.</p> <p>Such an object implements the Arrow C Stream interface via the Arrow PyCapsule Interface. This allows for zero-copy Arrow data interchange across libraries.</p>"},{"location":"api/core/types/#arro3.core.types.ArrowStreamExportable.__arrow_c_stream__","title":"__arrow_c_stream__","text":"<pre><code>__arrow_c_stream__(requested_schema: object | None = None) -&gt; object\n</code></pre>"},{"location":"api/core/types/#arro3.core.types.BufferProtocolExportable","title":"BufferProtocolExportable","text":"<p>               Bases: <code>Protocol</code></p> <p>A python object that implements the Buffer Protocol</p>"}]}