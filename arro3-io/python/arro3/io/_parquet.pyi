import sys
from pathlib import Path
from typing import IO, Literal, Protocol, Sequence, TypedDict

# Note: importing with
# `from arro3.core import Array`
# will cause Array to be included in the generated docs in this module.
import arro3.core as core
import arro3.core.types as types
from obstore.store import ObjectStore as ObstoreStore

from ._pyo3_object_store import ObjectStore
from ._stream import RecordBatchStream

if sys.version_info >= (3, 11):
    from typing import Unpack
else:
    from typing_extensions import Unpack

ParquetColumnPath = str | Sequence[str]
"""Allowed types to refer to a Parquet Column."""

ParquetCompression = (
    Literal["uncompressed", "snappy", "gzip", "lzo", "brotli", "lz4", "zstd", "lz4_raw"]
    | str
)
"""Allowed compression schemes for Parquet."""

ParquetEncoding = Literal[
    "plain",
    "plain_dictionary",
    "rle",
    "bit_packed",
    "delta_binary_packed",
    "delta_length_byte_array",
    "delta_byte_array",
    "rle_dictionary",
    "byte_stream_split",
]
"""Allowed Parquet encodings."""

def read_parquet(file: IO[bytes] | Path | str) -> core.RecordBatchReader:
    """Read a Parquet file to an Arrow RecordBatchReader

    Args:
        file: The input Parquet file path or buffer.

    Returns:
        The loaded Arrow data.
    """

async def read_parquet_async(path: str, *, store: ObjectStore) -> core.Table:
    """Read a Parquet file to an Arrow Table in an async fashion

    Args:
        path: The path to the Parquet file in the given store

    Returns:
        The loaded Arrow data.
    """

class ParquetPredicate(Protocol):
    """A predicate operating on RecordBatch.

    See RowFilter for more information on the use of this trait.
    """

    @property
    def columns(self) -> Sequence[str]:
        """Returns the columns required to evaluate this predicate.

        All projected columns will be provided in the batch passed to evaluate.
        """
    def evaluate(self, batch: core.RecordBatch) -> types.ArrowArrayExportable:
        """Evaluate this predicate for the given `RecordBatch`.

        Only the columns identified by `Self.columns` will be provided in the batch.

        Must return a boolean-typed `Array` that has the same length as the input batch
        where each row indicates whether the row should be returned:

        - `True`: the row should be returned
        - `False` or `null`: the row should not be returned
        """

class ParquetOpenOptions(TypedDict, total=False):
    """Options passed when opening `ParquetFile`."""

    store: ObjectStore | ObstoreStore | None
    """
    A store to use when opening Parquet files from cloud storage.
    """

    skip_arrow_metadata: bool
    """If `True`, skip decoding the embedded arrow metadata.

    Parquet files generated by some writers may contain embedded arrow schema and
    metadata. This may not be correct or compatible with your system, for example:
    [ARROW-16184](https://issues.apache.org/jira/browse/ARROW-16184).
    """

    schema: core.Schema | None
    """Provide a schema to use when reading the Parquet file.

    If provided it takes precedence over the schema inferred from the file or the schema
    defined in the file's metadata. If the schema is not compatible with the file's
    schema an error will be returned when constructing the builder.

    This option is only required if you want to cast columns to a different type. For
    example, if you wanted to cast from an `Int64` in the Parquet file to a `Timestamp`
    in the Arrow schema.

    The supplied schema must have the same number of columns as the Parquet schema and
    the column names need to be the same.
    """

    page_index: bool
    """Enable reading [`PageIndex`], if present.

    The `PageIndex` can be used to push down predicates to the parquet scan, potentially
    eliminating unnecessary IO, by some query engines.

    [PageIndex]: https://github.com/apache/parquet-format/blob/master/PageIndex.md
    """

class ParquetReadOptions(TypedDict, total=False):
    """Options passed to read calls of `ParquetFile`."""

    batch_size: int | None
    """Set the size of `RecordBatch` to produce.
    """
    row_groups: Sequence[int] | None
    """Only read data from the provided row group indexes.

    This is also called row group filtering.
    """
    columns: Sequence[str] | None
    """Only read data from the provided column indexes.
    """
    filters: ParquetPredicate | Sequence[ParquetPredicate] | None
    """Provide one or more filters to skip decoding rows.

    These filters are applied during the parquet read process **after** row group
    selection. <!-- and row selection. -->

    Filters are applied in order after decoding only the columns
    required. As filters eliminate rows, fewer rows from subsequent columns
    may be required, thus potentially reducing IO and decode.

    `filters` consists of one or more [`ParquetPredicate`s][arro3.io.ParquetPredicate].
    Only the rows for which all the predicates evaluate to `true` will be returned.

    <!--
    Any [`RowSelection`] provided to the reader will be applied prior to the first
    predicate, and each predicate in turn will then be used to compute a more refined
    [`RowSelection`] used when evaluating the subsequent predicates.

    Once all predicates have been evaluated, the final [`RowSelection`] is applied
    to the top-level [`ProjectionMask`] to produce the final output [`RecordBatch`].
    -->

    This design has a couple of implications:

    - filters can be used to skip entire pages, and thus IO, in addition to CPU decode overheads
    - Columns may be decoded multiple times if they appear in multiple predicates
    - IO will be deferred until needed by columns required by a predicate.

    As such there is a trade-off between a single large predicate, or multiple
    predicates, that will depend on the shape of the data. Whilst multiple smaller
    predicates may minimise the amount of data scanned/decoded, it may not be faster
    overall.

    For example, if a predicate that needs a single column of data filters out all but
    1% of the rows, applying it as one of the early predicates will likely significantly
    improve performance.

    As a counter example, if a predicate needs several columns of data to evaluate but
    leaves 99% of the rows, it may be better to not filter the data from parquet and
    apply the filter after the `RecordBatch` has been fully decoded.

    Additionally, even if a predicate eliminates a moderate number of rows, it may still
    be faster to filter the data after the RecordBatch has been fully decoded, if the
    eliminated rows are not contiguous.

    !!! note

        It is recommended to enable reading the page index if using this functionality,
        to allow more efficient skipping over data pages. See
        [`ParquetOpenOptions.page_index`][arro3.io.ParquetOpenOptions.page_index].
    """

    limit: int | None
    """Provide a limit to the number of rows to be read.

    <!-- row selection. -->

    The limit will be applied after any `filters` allowing it to limit the final set of
    rows decoded after any pushed down predicates.

    !!! note

        It is recommended to enable reading the page index if using this functionality,
        to allow more efficient skipping over data pages. See
        [`ParquetOpenOptions.page_index`][arro3.io.ParquetOpenOptions.page_index].
    """
    offset: int | None
    """Provide an offset to skip over the given number of rows.

    The offset will be applied after any `filters` allowing it to skip rows after any
    pushed down predicates.

    !!! note

        It is recommended to enable reading the page index if using this functionality,
        to allow more efficient skipping over data pages. See
        [`ParquetOpenOptions.page_index`][arro3.io.ParquetOpenOptions.page_index].
    """

class ParquetFile:
    @classmethod
    def open(
        cls,
        file: IO[bytes] | Path | str,
        **kwargs: Unpack[ParquetOpenOptions],
    ) -> ParquetFile:
        """Open a Parquet file."""

    @classmethod
    async def open_async(
        cls,
        file: IO[bytes] | Path | str,
        *,
        store: ObjectStore | ObstoreStore | None = None,
        skip_arrow_metadata: bool = False,
        schema: core.Schema | None = None,
        page_index: bool = False,
    ) -> ParquetFile:
        """Open a Parquet file."""

    @property
    def num_row_groups(self) -> int:
        """Return the number of row groups in the Parquet file."""

    def read(self, **kwargs: Unpack[ParquetReadOptions]) -> core.RecordBatchReader:
        """Read the Parquet file to an Arrow RecordBatchReader.

        Keyword Args:
            batch_size: The number of rows to read in each batch.
            row_groups: The row groups to read.
            columns: The columns to read.
            limit: The number of rows to read.
            offset: The number of rows to skip.

        Returns:
            The loaded Arrow data.
        """
    def read_async(self, **kwargs: Unpack[ParquetReadOptions]) -> RecordBatchStream:
        """Read the Parquet file to an Arrow async RecordBatchStream.

        Note that this method itself is **not async**, but returns an async iterable
        that yields RecordBatches.

        Keyword Args:
            batch_size: The number of rows to read in each batch.
            row_groups: The row groups to read.
            columns: The columns to read.
            limit: The number of rows to read.
            offset: The number of rows to skip.

        Returns:
            The loaded Arrow data.
        """
    @property
    def schema_arrow(self) -> core.Schema:
        """Return the Arrow schema of the Parquet file."""

def write_parquet(
    data: types.ArrowStreamExportable | types.ArrowArrayExportable,
    file: IO[bytes] | Path | str,
    *,
    bloom_filter_enabled: bool | None = None,
    bloom_filter_fpp: float | None = None,
    bloom_filter_ndv: int | None = None,
    column_compression: dict[ParquetColumnPath, ParquetCompression] | None = None,
    column_dictionary_enabled: dict[ParquetColumnPath, bool] | None = None,
    column_encoding: dict[ParquetColumnPath, ParquetEncoding] | None = None,
    column_max_statistics_size: dict[ParquetColumnPath, int] | None = None,
    compression: ParquetCompression | None = None,
    created_by: str | None = None,
    data_page_row_count_limit: int | None = None,
    data_page_size_limit: int | None = None,
    dictionary_enabled: bool | None = None,
    dictionary_page_size_limit: int | None = None,
    encoding: ParquetEncoding | None = None,
    key_value_metadata: dict[str, str] | None = None,
    max_row_group_size: int | None = None,
    max_statistics_size: int | None = None,
    skip_arrow_metadata: bool = False,
    write_batch_size: int | None = None,
    writer_version: Literal["parquet_1_0", "parquet_2_0"] | None = None,
) -> None:
    """Write an Arrow Table or stream to a Parquet file.

    Args:
        data: The Arrow Table, RecordBatchReader, or RecordBatch to write.
        file: The output file.

    Keyword Args:
        bloom_filter_enabled: Sets if bloom filter is enabled by default for all columns
            (defaults to `false`).
        bloom_filter_fpp: Sets the default target bloom filter false positive
            probability (fpp) for all columns (defaults to `0.05`).
        bloom_filter_ndv: Sets default number of distinct values (ndv) for bloom filter
            for all columns (defaults to `1_000_000`).
        column_compression: Sets compression codec for a specific column. Takes
            precedence over `compression`.
        column_dictionary_enabled: Sets flag to enable/disable dictionary encoding for a
            specific column. Takes precedence over `dictionary_enabled`.
        column_encoding: Sets encoding for a specific column. Takes precedence over
            `encoding`.
        column_max_statistics_size: Sets max size for statistics for a specific column.
            Takes precedence over `max_statistics_size`.
        compression:
            Sets default compression codec for all columns (default to `uncompressed`).
            Note that you can pass in a custom compression level with a string like
            `"zstd(3)"` or `"gzip(9)"` or `"brotli(3)"`.
        created_by: Sets "created by" property (defaults to `parquet-rs version
            <VERSION>`).
        data_page_row_count_limit:
            Sets best effort maximum number of rows in a data page (defaults to
            `20_000`).

            The parquet writer will attempt to limit the number of rows in each
            `DataPage` to this value. Reducing this value will result in larger parquet
            files, but may improve the effectiveness of page index based predicate
            pushdown during reading.

            Note: this is a best effort limit based on value of `set_write_batch_size`.

        data_page_size_limit:
            Sets best effort maximum size of a data page in bytes (defaults to `1024 *
            1024`).

            The parquet writer will attempt to limit the sizes of each `DataPage` to
            this many bytes. Reducing this value will result in larger parquet files,
            but may improve the effectiveness of page index based predicate pushdown
            during reading.

            Note: this is a best effort limit based on value of `set_write_batch_size`.
        dictionary_enabled: Sets default flag to enable/disable dictionary encoding for
            all columns (defaults to `True`).
        dictionary_page_size_limit:
            Sets best effort maximum dictionary page size, in bytes (defaults to `1024 *
            1024`).

            The parquet writer will attempt to limit the size of each `DataPage` used to
            store dictionaries to this many bytes. Reducing this value will result in
            larger parquet files, but may improve the effectiveness of page index based
            predicate pushdown during reading.

            Note: this is a best effort limit based on value of `set_write_batch_size`.

        encoding:
            Sets default encoding for all columns.

            If dictionary is not enabled, this is treated as a primary encoding for all
            columns. In case when dictionary is enabled for any column, this value is
            considered to be a fallback encoding for that column.
        key_value_metadata: Sets "key_value_metadata" property (defaults to `None`).
        max_row_group_size: Sets maximum number of rows in a row group (defaults to
            `1024 * 1024`).
        max_statistics_size: Sets default max statistics size for all columns (defaults
            to `4096`).
        skip_arrow_metadata: Parquet files generated by this writer contain embedded
            arrow schema by default. Set `skip_arrow_metadata` to `True`, to skip
            encoding the embedded metadata (defaults to `False`).
        write_batch_size:
            Sets write batch size (defaults to 1024).

            For performance reasons, data for each column is written in batches of this
            size.

            Additional limits such as such as `set_data_page_row_count_limit` are
            checked between batches, and thus the write batch size value acts as an
            upper-bound on the enforcement granularity of other limits.
        writer_version: Sets the `WriterVersion` written into the parquet metadata
            (defaults to `"parquet_1_0"`). This value can determine what features some
            readers will support.

    """
